{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import os.path\n",
    "\n",
    "def load_babi_data(is_training, task, embedding_dim, embedding, word_id_dict):\n",
    "    if is_training:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\"\n",
    "    else:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_test.txt\"\n",
    "    \n",
    "    file = open(filepath)\n",
    "    num_words_in_longest_input_sentence = 0\n",
    "    num_words_in_longest_question = 0\n",
    "    num_sentences_in_each_chapter = []\n",
    "    chapter_input = []\n",
    "    data = []\n",
    "    \n",
    "    for line in file:\n",
    "        items = re.sub('[?.]', '', line).lower().split()\n",
    "        if items[-1].isdigit():\n",
    "            data.append({'I': copy.deepcopy(chapter_input),\n",
    "                     'Q': items[1:-2],\n",
    "                     'A': [items[-2]]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "            num_words_in_longest_question = max(num_words_in_longest_question, len(items[1:-2]))\n",
    "        else:\n",
    "            if items[0] == '1':\n",
    "                chapter_input = [items[1:]]\n",
    "            else:\n",
    "                chapter_input.append(items[1:])\n",
    "            num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, len(items[1:]))\n",
    "    file.close()\n",
    "\n",
    "    num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "    num_chapters = len(data)\n",
    "\n",
    "    data_inputs = np.zeros([num_chapters, num_sentences_in_longest_input, num_words_in_longest_input_sentence, embedding_dim])\n",
    "    data_questions = np.zeros([num_chapters, num_words_in_longest_question, embedding_dim])\n",
    "    data_answers = np.zeros([num_chapters])\n",
    "    for chapter_index, chapter in enumerate(data):\n",
    "        for sentence_index, sentence in enumerate(chapter['I']):\n",
    "            data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = embedding[[word_id_dict[word] for word in sentence]]\n",
    "        data_questions[chapter_index, 0:len(chapter['Q']), :] = embedding[[word_id_dict[word] for word in chapter['Q']]]\n",
    "        data_answers[chapter_index] = word_id_dict[chapter['A'][0]]\n",
    "    \n",
    "    return(data_inputs, data_questions, data_answers, \n",
    "           num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "           num_words_in_longest_question, num_sentences_in_longest_input, \n",
    "           num_chapters)\n",
    "\n",
    "def load_glove():\n",
    "    vocab_size = 400000\n",
    "    embedding_dim = 50\n",
    "    file = open(\"../../../datasets/glove_6b/glove.6B.50d.txt\")    \n",
    "    embedding = np.ndarray([vocab_size, embedding_dim])\n",
    "    word_id_dict = {}\n",
    "    id_word_dict = {}\n",
    "    id = 0\n",
    "    for line in file:\n",
    "        items = line.split(' ')\n",
    "        word_id_dict[items[0]] = id\n",
    "        id_word_dict[id] = items[0]\n",
    "        embedding[id,:] = np.array([float(i) for i in items[1:]])\n",
    "        id += 1\n",
    "    file.close()\n",
    "    return(embedding, word_id_dict, id_word_dict, vocab_size, embedding_dim)\n",
    "\n",
    "def create_position_encoding(embedding_dim, num_words_in_longest_input_sentence):\n",
    "    ## Position encoding\n",
    "    position_encoding = np.ones([embedding_dim, num_words_in_longest_input_sentence], dtype=np.float32)\n",
    "\n",
    "    ## Below (my implementation, from section 3.1 in https://arxiv.org/pdf/1603.01417.pdf) didn't work.\n",
    "    # for j in range(1, num_words_in_longest_input_sentence+1):\n",
    "    #     for d in range(1, embedding_dim+1):\n",
    "    #         position_encoding[d-1, j-1] = (1 - j/num_words_in_longest_input_sentence) - (d/embedding_dim)*(1 - 2*j/num_words_in_longest_input_sentence)\n",
    "\n",
    "    ## Copied from https://github.com/domluna/memn2n\n",
    "    ls = num_words_in_longest_input_sentence+1\n",
    "    le = embedding_dim+1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            position_encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)\n",
    "    position_encoding = 1 + 4 * position_encoding / embedding_dim / num_words_in_longest_input_sentence\n",
    "    position_encoding = np.transpose(position_encoding)\n",
    "    return(position_encoding)\n",
    "\n",
    "\n",
    "class dmn_plus:\n",
    "    def __init__(self):\n",
    "        self.hidden_layer_size = 80\n",
    "        self.num_steps = 3\n",
    "        self.batch_size = 100\n",
    "        self.dropout_probability = 0.9\n",
    "        self.l2_regularization_lambda = 0.001\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 100\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        self.embedding, self.word_id_dict, self.id_word_dict, self.vocab_size, self.embedding_dim = load_glove()\n",
    "        \n",
    "    def load_data(self, is_training = True, task = 1):\n",
    "        self.task = task\n",
    "        self.data_inputs, self.data_questions, self.data_answers, self.num_sentences_in_each_chapter, self.num_words_in_longest_input_sentence, self.num_words_in_longest_question, self.num_sentences_in_longest_input, self.num_chapters = load_babi_data(is_training, self.task, self.embedding_dim, self.embedding, self.word_id_dict)\n",
    "        self.position_encoding = create_position_encoding(self.embedding_dim, self.num_words_in_longest_input_sentence)\n",
    "        \n",
    "    def answer_user_data(self, inputs, questions):    \n",
    "        # load model on user data\n",
    "        self.data_inputs, self.data_questions, self.data_answers, self.num_sentences_in_each_chapter, self.num_words_in_longest_input_sentence, self.num_words_in_longest_question, self.num_sentences_in_longest_input, self.num_chapters = self.prepare_user_inputted_data(inputs, questions)\n",
    "        self.position_encoding = create_position_encoding(self.embedding_dim, self.num_words_in_longest_input_sentence)\n",
    "        # get predictions\n",
    "        predictions = self.sess.run(self.predictions, feed_dict = self.get_batch(0))\n",
    "        predictions = [self.id_word_dict[id] for id in predictions]\n",
    "        return(predictions)\n",
    "    \n",
    "    def prepare_user_inputted_data(self, inputs, questions):    \n",
    "        num_words_in_longest_input_sentence = 0\n",
    "        num_words_in_longest_question = 0\n",
    "        num_sentences_in_each_chapter = []\n",
    "        chapter_input = []\n",
    "        data = []\n",
    "\n",
    "        for index in range(len(inputs)):\n",
    "            input = re.sub('[?]', '', inputs[index]).lower().split('.')\n",
    "            chapter_input = []\n",
    "            for sentence in input:\n",
    "                if sentence != '':\n",
    "                    chapter_input.append(sentence.split())\n",
    "            cleaned_question = re.sub('[?]', '', questions[index]).lower().split()\n",
    "            data.append({'I': chapter_input,\n",
    "                         'Q': cleaned_question,\n",
    "                         'A': [None]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "            num_words_in_longest_question = max(num_words_in_longest_question, len(cleaned_question))\n",
    "            num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, max([len(sentence) for sentence in chapter_input]))\n",
    "        num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "        num_chapters = len(data)\n",
    "\n",
    "        data_inputs = np.zeros([num_chapters, 10, 6, self.embedding_dim])\n",
    "        data_questions = np.zeros([num_chapters, num_words_in_longest_question, self.embedding_dim])\n",
    "        data_answers = np.zeros([num_chapters])\n",
    "        for chapter_index, chapter in enumerate(data):\n",
    "            for sentence_index, sentence in enumerate(chapter['I']):\n",
    "                data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = self.embedding[[self.word_id_dict[word] for word in sentence]]\n",
    "            data_questions[chapter_index, 0:len(chapter['Q']), :] = self.embedding[[self.word_id_dict[word] for word in chapter['Q']]]\n",
    "            data_answers[chapter_index] = None\n",
    "\n",
    "        return(data_inputs, data_questions, data_answers, \n",
    "               num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "               num_words_in_longest_question, num_sentences_in_longest_input, \n",
    "               num_chapters)\n",
    "    \n",
    "#     def get_answers_for_user_inputted_data\n",
    "        \n",
    "    def get_batch(self, batch_number):\n",
    "        return {self.inputs: self.data_inputs[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.questions: self.data_questions[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.answers: self.data_answers[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.input_lengths: self.num_sentences_in_each_chapter[batch_number*self.batch_size: (batch_number+1)*self.batch_size]\n",
    "               }\n",
    "    \n",
    "    def random_entry_from_data(self):\n",
    "        idx = np.random.randint(0, self.num_chapters)\n",
    "        return {self.inputs: np.expand_dims(self.data_inputs[idx], axis = 0),\n",
    "                self.questions: np.expand_dims(self.data_questions[idx], axis = 0),\n",
    "                self.answers: np.expand_dims(self.data_answers[idx], axis = 0),\n",
    "                self.input_lengths: np.expand_dims(self.num_sentences_in_each_chapter[idx], axis = 0)\n",
    "               }\n",
    "    \n",
    "    def sgd_train(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss, epoch_num_correct, _ = self.sess.run((self.loss, self.num_correct, self.optimizer), \n",
    "                                                                            feed_dict = self.random_entry_from_data())\n",
    "            if epoch%100 == 0:\n",
    "                print(\"Epoch %d: %.2f%% complete, %d mins, Loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/self.num_chapters))\n",
    "        end_time = time.time()\n",
    "        print(\"Duration: %d mins\" % int((end_time - start_time)/60))\n",
    "                \n",
    "    def train(self):            \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = epoch_num_correct = 0\n",
    "            for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "                batch_loss, batch_num_correct, _ = self.sess.run((self.loss, self.num_correct, self.optimizer), \n",
    "                                                                            feed_dict = self.get_batch(batch_idx))\n",
    "                epoch_loss += batch_loss\n",
    "                epoch_num_correct += batch_num_correct\n",
    "            print(\"Epoch %d: %.2f%% complete, %d mins, Loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/self.num_chapters))\n",
    "        end_time = time.time()\n",
    "        print(\"Duration: %d mins\" % int((end_time - start_time)/60))\n",
    "        \n",
    "    def save(self):\n",
    "        # create filename based on task\n",
    "        save_path = self.saver.save(self.sess, \"../saved-models/task-%d.ckpt\"%self.task)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "    def restore(self, task):\n",
    "        # check if there is any saved model for that particular task\n",
    "        if os.path.isfile(\"../saved-models/task-%d.ckpt.meta\"%task):\n",
    "            self.saver.restore(self.sess, \"../saved-models/task-%d.ckpt\"%task)\n",
    "            print(\"Model restored\")\n",
    "        else:\n",
    "            print(\"Saved model for given task does not exist\")\n",
    "        \n",
    "    def test(self):\n",
    "        start_time = time.time()\n",
    "        total_num_correct = 0\n",
    "        for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "            batch_num_correct = self.sess.run(self.num_correct, feed_dict = self.get_batch(batch_idx))\n",
    "            total_num_correct += batch_num_correct\n",
    "        print(\"%d mins, Num correct: %d, Accuracy: %.2f%%\" % ((time.time() - start_time)/60,\n",
    "                                                              total_num_correct,\n",
    "                                                              total_num_correct*100.0/self.num_chapters))\n",
    "        \n",
    "    def create_tensorflow_graph(self):\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, 10, 6, self.embedding_dim])\n",
    "        self.questions = tf.placeholder(tf.float32, [None, None, self.embedding_dim])\n",
    "        self.answers = tf.placeholder(tf.int32, [None])\n",
    "        self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        ## Question module\n",
    "        with tf.variable_scope('question_module'):\n",
    "            question_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            _, self.question_vector = tf.nn.dynamic_rnn(question_gru_cell,\n",
    "                                                  self.questions,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "        ## Input module\n",
    "        with tf.variable_scope('input_module'):\n",
    "\n",
    "            positionally_encoded_inputs = tf.reduce_sum(self.inputs*self.position_encoding, 2)\n",
    "\n",
    "            input_forward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_backward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_module_output, _ = tf.nn.bidirectional_dynamic_rnn(input_forward_gru_cell,\n",
    "                                                                    input_backward_gru_cell,\n",
    "                                                                    positionally_encoded_inputs,\n",
    "                                                                    sequence_length = self.input_lengths,\n",
    "                                                                    dtype = tf.float32)\n",
    "            input_fact_vectors = tf.add(input_module_output[0], input_module_output[1])\n",
    "            input_fact_vectors = tf.nn.dropout(input_fact_vectors, self.dropout_probability)\n",
    "\n",
    "        ## Episodic Memory module\n",
    "        with tf.variable_scope('episodic_memory_module'):\n",
    "            weight = tf.get_variable(\"weight\", [3*self.hidden_layer_size, 80],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            bias = tf.get_variable(\"bias\", [1, self.hidden_layer_size],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            self.previous_memory = self.question_vector\n",
    "            for step in range(self.num_steps):\n",
    "                attentions = []\n",
    "                for fact_index, fact_vector in enumerate(tf.unstack(input_fact_vectors, axis = 1)):\n",
    "#                     if fact_index == 0:\n",
    "                    reuse = bool(step) or bool(fact_index)\n",
    "                    with tf.variable_scope(\"attention\", reuse = reuse):\n",
    "                        z = tf.concat([tf.multiply(fact_vector, self.question_vector), \n",
    "                                       tf.multiply(fact_vector, self.previous_memory),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.question_vector)),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.previous_memory))], 1)\n",
    "                        attention = tf.contrib.layers.fully_connected(z,\n",
    "                                                                    self.embedding_dim,\n",
    "                                                                    activation_fn=tf.nn.tanh,\n",
    "                                                                    reuse=reuse, scope=\"fc1\")\n",
    "                        attention = tf.contrib.layers.fully_connected(attention,\n",
    "                                                                    1,\n",
    "                                                                    activation_fn=None,\n",
    "                                                                    reuse=reuse, scope=\"fc2\")\n",
    "                        attentions.append(tf.squeeze(attention))\n",
    "                attentions = tf.expand_dims(tf.nn.softmax(tf.transpose(tf.stack(attentions))), axis=-1)\n",
    "                reuse = True if step > 0 else False\n",
    "                # soft attention\n",
    "                self.context_vector = tf.reduce_sum(tf.multiply(input_fact_vectors, attentions), axis = 1)\n",
    "#                 with tf.variable_scope(\"step%d\"%step):\n",
    "                self.previous_memory = tf.nn.relu(tf.matmul(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1), \n",
    "                                                            weight) + bias)\n",
    "#                     self.previous_memory = tf.contrib.layers.fully_connected(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1),\n",
    "#                                                                                 self.hidden_layer_size,\n",
    "#                                                                                 activation_fn=tf.nn.relu)\n",
    "                        \n",
    "            self.previous_memory = tf.nn.dropout(self.previous_memory, self.dropout_probability)\n",
    "\n",
    "        ## Answer module\n",
    "        with tf.variable_scope('answer_module') as scope:\n",
    "            logits = tf.contrib.layers.fully_connected(inputs = tf.concat([self.previous_memory, self.question_vector], axis = 1),\n",
    "                                                      num_outputs = self.vocab_size,\n",
    "                                                      activation_fn = None)\n",
    "\n",
    "            ## Loss and metrics\n",
    "            self.loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = self.answers))\n",
    "\n",
    "            # add l2 regularization for all variables except biases\n",
    "            for v in tf.trainable_variables():\n",
    "                if not 'bias' in v.name.lower():\n",
    "                    self.loss += self.l2_regularization_lambda * tf.nn.l2_loss(v)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            self.predictions = tf.cast(tf.argmax(tf.nn.softmax(logits), 1), 'int32')\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(tf.equal(self.predictions, self.answers), tf.int32))\n",
    "            \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = dmn_plus()\n",
    "model.load_embeddings()\n",
    "model.load_data(is_training=True, task=1)\n",
    "model.create_tensorflow_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.00% complete, 0 mins, Loss: 4928.28, Num correct: 125, Accuracy: 12.50%\n",
      "Epoch 1: 1.00% complete, 0 mins, Loss: 2194.60, Num correct: 166, Accuracy: 16.60%\n",
      "Epoch 2: 2.00% complete, 0 mins, Loss: 1979.15, Num correct: 168, Accuracy: 16.80%\n",
      "Epoch 3: 3.00% complete, 0 mins, Loss: 1906.07, Num correct: 213, Accuracy: 21.30%\n",
      "Epoch 4: 4.00% complete, 0 mins, Loss: 1872.77, Num correct: 233, Accuracy: 23.30%\n",
      "Epoch 5: 5.00% complete, 0 mins, Loss: 1856.47, Num correct: 253, Accuracy: 25.30%\n",
      "Epoch 6: 6.00% complete, 0 mins, Loss: 1807.16, Num correct: 293, Accuracy: 29.30%\n",
      "Epoch 7: 7.00% complete, 0 mins, Loss: 1754.40, Num correct: 350, Accuracy: 35.00%\n",
      "Epoch 8: 8.00% complete, 0 mins, Loss: 1672.41, Num correct: 361, Accuracy: 36.10%\n",
      "Epoch 9: 9.00% complete, 0 mins, Loss: 1600.53, Num correct: 418, Accuracy: 41.80%\n",
      "Epoch 10: 10.00% complete, 0 mins, Loss: 1583.41, Num correct: 422, Accuracy: 42.20%\n",
      "Epoch 11: 11.00% complete, 0 mins, Loss: 1515.85, Num correct: 444, Accuracy: 44.40%\n",
      "Epoch 12: 12.00% complete, 0 mins, Loss: 1434.27, Num correct: 504, Accuracy: 50.40%\n",
      "Epoch 13: 13.00% complete, 0 mins, Loss: 1409.76, Num correct: 517, Accuracy: 51.70%\n",
      "Epoch 14: 14.00% complete, 0 mins, Loss: 1412.97, Num correct: 511, Accuracy: 51.10%\n",
      "Epoch 15: 15.00% complete, 0 mins, Loss: 1397.39, Num correct: 518, Accuracy: 51.80%\n",
      "Epoch 16: 16.00% complete, 0 mins, Loss: 1361.32, Num correct: 521, Accuracy: 52.10%\n",
      "Epoch 17: 17.00% complete, 0 mins, Loss: 1301.69, Num correct: 548, Accuracy: 54.80%\n",
      "Epoch 18: 18.00% complete, 0 mins, Loss: 1230.74, Num correct: 578, Accuracy: 57.80%\n",
      "Epoch 19: 19.00% complete, 0 mins, Loss: 1205.49, Num correct: 613, Accuracy: 61.30%\n",
      "Epoch 20: 20.00% complete, 0 mins, Loss: 1176.71, Num correct: 610, Accuracy: 61.00%\n",
      "Epoch 21: 21.00% complete, 0 mins, Loss: 1096.33, Num correct: 622, Accuracy: 62.20%\n",
      "Epoch 22: 22.00% complete, 0 mins, Loss: 1069.31, Num correct: 663, Accuracy: 66.30%\n",
      "Epoch 23: 23.00% complete, 0 mins, Loss: 960.03, Num correct: 698, Accuracy: 69.80%\n",
      "Epoch 24: 24.00% complete, 0 mins, Loss: 828.73, Num correct: 745, Accuracy: 74.50%\n",
      "Epoch 25: 25.00% complete, 0 mins, Loss: 767.02, Num correct: 772, Accuracy: 77.20%\n",
      "Epoch 26: 26.00% complete, 0 mins, Loss: 685.31, Num correct: 802, Accuracy: 80.20%\n",
      "Epoch 27: 27.00% complete, 0 mins, Loss: 583.45, Num correct: 836, Accuracy: 83.60%\n",
      "Epoch 28: 28.00% complete, 0 mins, Loss: 426.08, Num correct: 888, Accuracy: 88.80%\n",
      "Epoch 29: 29.00% complete, 0 mins, Loss: 424.62, Num correct: 904, Accuracy: 90.40%\n",
      "Epoch 30: 30.00% complete, 0 mins, Loss: 395.54, Num correct: 910, Accuracy: 91.00%\n",
      "Epoch 31: 31.00% complete, 0 mins, Loss: 330.03, Num correct: 932, Accuracy: 93.20%\n",
      "Epoch 32: 32.00% complete, 0 mins, Loss: 323.67, Num correct: 936, Accuracy: 93.60%\n",
      "Epoch 33: 33.00% complete, 0 mins, Loss: 350.00, Num correct: 932, Accuracy: 93.20%\n",
      "Epoch 34: 34.00% complete, 0 mins, Loss: 273.63, Num correct: 944, Accuracy: 94.40%\n",
      "Epoch 35: 35.00% complete, 0 mins, Loss: 265.01, Num correct: 939, Accuracy: 93.90%\n",
      "Epoch 36: 36.00% complete, 0 mins, Loss: 210.77, Num correct: 971, Accuracy: 97.10%\n",
      "Epoch 37: 37.00% complete, 0 mins, Loss: 216.62, Num correct: 967, Accuracy: 96.70%\n",
      "Epoch 38: 38.00% complete, 0 mins, Loss: 235.38, Num correct: 963, Accuracy: 96.30%\n",
      "Epoch 39: 39.00% complete, 0 mins, Loss: 233.69, Num correct: 961, Accuracy: 96.10%\n",
      "Epoch 40: 40.00% complete, 0 mins, Loss: 206.27, Num correct: 971, Accuracy: 97.10%\n",
      "Epoch 41: 41.00% complete, 0 mins, Loss: 190.32, Num correct: 977, Accuracy: 97.70%\n",
      "Epoch 42: 42.00% complete, 0 mins, Loss: 175.05, Num correct: 976, Accuracy: 97.60%\n",
      "Epoch 43: 43.00% complete, 0 mins, Loss: 187.61, Num correct: 977, Accuracy: 97.70%\n",
      "Epoch 44: 44.00% complete, 0 mins, Loss: 183.07, Num correct: 975, Accuracy: 97.50%\n",
      "Epoch 45: 45.00% complete, 0 mins, Loss: 156.13, Num correct: 982, Accuracy: 98.20%\n",
      "Epoch 46: 46.00% complete, 0 mins, Loss: 171.63, Num correct: 979, Accuracy: 97.90%\n",
      "Epoch 47: 47.00% complete, 0 mins, Loss: 169.06, Num correct: 984, Accuracy: 98.40%\n",
      "Epoch 48: 48.00% complete, 0 mins, Loss: 162.42, Num correct: 984, Accuracy: 98.40%\n",
      "Epoch 49: 49.00% complete, 0 mins, Loss: 140.08, Num correct: 984, Accuracy: 98.40%\n",
      "Epoch 50: 50.00% complete, 0 mins, Loss: 173.03, Num correct: 984, Accuracy: 98.40%\n",
      "Epoch 51: 51.00% complete, 1 mins, Loss: 222.41, Num correct: 978, Accuracy: 97.80%\n",
      "Epoch 52: 52.00% complete, 1 mins, Loss: 210.99, Num correct: 972, Accuracy: 97.20%\n",
      "Epoch 53: 53.00% complete, 1 mins, Loss: 150.91, Num correct: 984, Accuracy: 98.40%\n",
      "Epoch 54: 54.00% complete, 1 mins, Loss: 149.01, Num correct: 985, Accuracy: 98.50%\n",
      "Epoch 55: 55.00% complete, 1 mins, Loss: 153.27, Num correct: 977, Accuracy: 97.70%\n",
      "Epoch 56: 56.00% complete, 1 mins, Loss: 163.23, Num correct: 978, Accuracy: 97.80%\n",
      "Epoch 57: 57.00% complete, 1 mins, Loss: 145.52, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 58: 58.00% complete, 1 mins, Loss: 160.96, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 59: 59.00% complete, 1 mins, Loss: 147.01, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 60: 60.00% complete, 1 mins, Loss: 142.68, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 61: 61.00% complete, 1 mins, Loss: 123.17, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 62: 62.00% complete, 1 mins, Loss: 126.21, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 63: 63.00% complete, 1 mins, Loss: 124.87, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 64: 64.00% complete, 1 mins, Loss: 146.10, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 65: 65.00% complete, 1 mins, Loss: 118.44, Num correct: 995, Accuracy: 99.50%\n",
      "Epoch 66: 66.00% complete, 1 mins, Loss: 122.85, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 67: 67.00% complete, 1 mins, Loss: 148.52, Num correct: 989, Accuracy: 98.90%\n",
      "Epoch 68: 68.00% complete, 1 mins, Loss: 120.49, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 69: 69.00% complete, 1 mins, Loss: 129.23, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 70: 70.00% complete, 1 mins, Loss: 122.60, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 71: 71.00% complete, 1 mins, Loss: 133.67, Num correct: 989, Accuracy: 98.90%\n",
      "Epoch 72: 72.00% complete, 1 mins, Loss: 153.23, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 73: 73.00% complete, 1 mins, Loss: 145.61, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 74: 74.00% complete, 1 mins, Loss: 122.31, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 75: 75.00% complete, 1 mins, Loss: 127.72, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 76: 76.00% complete, 1 mins, Loss: 123.71, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 77: 77.00% complete, 1 mins, Loss: 160.38, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 78: 78.00% complete, 1 mins, Loss: 132.72, Num correct: 989, Accuracy: 98.90%\n",
      "Epoch 79: 79.00% complete, 1 mins, Loss: 118.47, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 80: 80.00% complete, 1 mins, Loss: 125.45, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 81: 81.00% complete, 1 mins, Loss: 120.60, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 82: 82.00% complete, 1 mins, Loss: 155.20, Num correct: 989, Accuracy: 98.90%\n",
      "Epoch 83: 83.00% complete, 1 mins, Loss: 142.55, Num correct: 989, Accuracy: 98.90%\n",
      "Epoch 84: 84.00% complete, 1 mins, Loss: 170.52, Num correct: 983, Accuracy: 98.30%\n",
      "Epoch 85: 85.00% complete, 1 mins, Loss: 166.65, Num correct: 980, Accuracy: 98.00%\n",
      "Epoch 86: 86.00% complete, 1 mins, Loss: 116.58, Num correct: 995, Accuracy: 99.50%\n",
      "Epoch 87: 87.00% complete, 1 mins, Loss: 109.85, Num correct: 997, Accuracy: 99.70%\n",
      "Epoch 88: 88.00% complete, 1 mins, Loss: 108.37, Num correct: 998, Accuracy: 99.80%\n",
      "Epoch 89: 89.00% complete, 1 mins, Loss: 114.12, Num correct: 996, Accuracy: 99.60%\n",
      "Epoch 90: 90.00% complete, 1 mins, Loss: 141.67, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 91: 91.00% complete, 1 mins, Loss: 134.03, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 92: 92.00% complete, 1 mins, Loss: 156.34, Num correct: 991, Accuracy: 99.10%\n",
      "Epoch 93: 93.00% complete, 1 mins, Loss: 140.69, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 94: 94.00% complete, 1 mins, Loss: 133.17, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 95: 95.00% complete, 1 mins, Loss: 127.66, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 96: 96.00% complete, 1 mins, Loss: 118.16, Num correct: 997, Accuracy: 99.70%\n",
      "Epoch 97: 97.00% complete, 1 mins, Loss: 114.48, Num correct: 994, Accuracy: 99.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: 98.00% complete, 1 mins, Loss: 111.64, Num correct: 995, Accuracy: 99.50%\n",
      "Epoch 99: 99.00% complete, 1 mins, Loss: 122.54, Num correct: 993, Accuracy: 99.30%\n",
      "Duration: 1 mins\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get word answer for input-question pairs (in word form) for GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hallway']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.answer_user_data(inputs = [\"John went to the bathroom. Mary went to the office. Adam went to the garden. Mary returned to the hallway.\"], questions = [\"Where is Mary?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 80)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sess.run(tf.global_variables_initializer())\n",
    "res = model.sess.run((model.previous_memory), feed_dict = model.get_batch(0))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ../saved-models/task-1.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mins, Num correct: 730, Accuracy: 73.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_data(is_training=False, task=1)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../saved-models/task-1.ckpt\n",
      "Model restored\n",
      "0 mins, Num correct: 990, Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "model = dmn_plus()\n",
    "model.load_embeddings()\n",
    "model.load_data(is_training=False, task=1)\n",
    "model.create_tensorflow_graph()\n",
    "model.restore(task = 1)\n",
    "model.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
