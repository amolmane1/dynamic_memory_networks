{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class dmn_plus:\n",
    "    def __init__(self, task):\n",
    "        self.vocab_size = 400000\n",
    "        self.embedding_dim = 50\n",
    "        self.hidden_layer_size = 80\n",
    "        self.num_steps = 3\n",
    "        self.batch_size = 100\n",
    "        self.dropout_probability = 0.9\n",
    "        self.l2_regularization_lambda = 0.001\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 128\n",
    "        self.num_epochs_before_checking_valid_loss = 5\n",
    "        self.num_consecutive_strips_before_stopping = 10\n",
    "        self.datatype_id_dict = {'train': 0, 'valid': 1, 'test': 2, 'user': 3}\n",
    "        self.datatypes = ['train', 'valid', 'test', 'user']\n",
    "        self.load_embeddings()\n",
    "        self.task = task\n",
    "        self.load_data(self.task)\n",
    "        self.create_tensorflow_graph()\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        file = open(\"../../../datasets/glove_6b/glove.6B.50d.txt\")    \n",
    "        self.embedding = np.ndarray([self.vocab_size, self.embedding_dim])\n",
    "        self.word_id_dict = {}\n",
    "        self.id_word_dict = {}\n",
    "        id = 0\n",
    "        for line in file:\n",
    "            items = line.split(' ')\n",
    "            self.word_id_dict[items[0]] = id\n",
    "            self.id_word_dict[id] = items[0]\n",
    "            self.embedding[id,:] = np.array([float(i) for i in items[1:]])\n",
    "            id += 1\n",
    "        file.close()\n",
    "        \n",
    "    def load_babi_data(self, datatype_id):\n",
    "        path_to_file_directory = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en-valid-10k/\"\n",
    "        path_to_file = path_to_file_directory + 'qa' + str(self.task) + '_' + self.datatypes[datatype_id] + '.txt'\n",
    "        \n",
    "        file = open(path_to_file)\n",
    "        num_words_in_longest_input_sentence = 0\n",
    "        num_words_in_longest_question = 0\n",
    "        num_sentences_in_each_chapter = []\n",
    "        chapter_input = []\n",
    "        data = []\n",
    "\n",
    "        for line in file:\n",
    "            items = re.sub('[?.]', '', line).lower().split()\n",
    "            if items[-1].isdigit():\n",
    "                # find the index of the second digit in that line\n",
    "                index_of_second_digit = len(items)\n",
    "                for index in range(len(items)-1, 0, -1):\n",
    "                    if items[index].isdigit():\n",
    "                        index_of_second_digit = index\n",
    "                data.append({'I': copy.deepcopy(chapter_input),\n",
    "                         'Q': items[1:index_of_second_digit-1],\n",
    "                         'A': [items[index_of_second_digit-1]]})\n",
    "                num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "                num_words_in_longest_question = max(num_words_in_longest_question, len(items[1:index_of_second_digit-1]))\n",
    "            else:\n",
    "                if items[0] == '1':\n",
    "                    chapter_input = [items[1:]]\n",
    "                else:\n",
    "                    chapter_input.append(items[1:])\n",
    "                num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, len(items[1:]))\n",
    "        file.close()\n",
    "\n",
    "        num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "        num_chapters = len(data)\n",
    "\n",
    "        return([data, num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "              num_words_in_longest_question, num_sentences_in_longest_input, num_chapters])\n",
    "    \n",
    "    def embed_and_pad_data(self, datatype_id):\n",
    "        num_chapters = self.data_and_metadata[datatype_id][5]\n",
    "        data_inputs = np.zeros([num_chapters, self.num_sentences_in_longest_input, self.num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        data_questions = np.zeros([num_chapters, self.num_words_in_longest_question, self.embedding_dim])\n",
    "        data_answers = np.zeros([num_chapters])\n",
    "        for chapter_index, chapter in enumerate(self.data_and_metadata[datatype_id][0]):\n",
    "            for sentence_index, sentence in enumerate(chapter['I']):\n",
    "                data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = self.embedding[[self.word_id_dict[word] for word in sentence]]\n",
    "            data_questions[chapter_index, 0:len(chapter['Q']), :] = self.embedding[[self.word_id_dict[word] for word in chapter['Q']]]\n",
    "            data_answers[chapter_index] = None if chapter['A'][0] == None else self.word_id_dict[chapter['A'][0]]\n",
    "            \n",
    "        return([data_inputs, data_questions, data_answers])\n",
    "    \n",
    "    def create_position_encoding(self):\n",
    "        self.position_encoding = np.ones([self.embedding_dim, self.num_words_in_longest_input_sentence], dtype=np.float32)\n",
    "\n",
    "        ## Below (my implementation, from section 3.1 in https://arxiv.org/pdf/1603.01417.pdf) didn't work.\n",
    "        # for j in range(1, num_words_in_longest_input_sentence+1):\n",
    "        #     for d in range(1, embedding_dim+1):\n",
    "        #         position_encoding[d-1, j-1] = (1 - j/num_words_in_longest_input_sentence) - (d/embedding_dim)*(1 - 2*j/num_words_in_longest_input_sentence)\n",
    "\n",
    "        ## Copied from https://github.com/domluna/memn2n\n",
    "        ls = self.num_words_in_longest_input_sentence+1\n",
    "        le = self.embedding_dim+1\n",
    "        for i in range(1, le):\n",
    "            for j in range(1, ls):\n",
    "                self.position_encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)\n",
    "        self.position_encoding = 1 + 4 * self.position_encoding / self.embedding_dim / self.num_words_in_longest_input_sentence\n",
    "        self.position_encoding = np.transpose(self.position_encoding)\n",
    "        \n",
    "    def load_data(self, task):\n",
    "        self.data_and_metadata = [self.load_babi_data(datatype_id = i) for i in range(3)]      \n",
    "        self.data_and_metadata.append(self.preprocess_user_data())\n",
    "        \n",
    "        self.num_words_in_longest_input_sentence = max([self.data_and_metadata[i][2] for i in range(3)])\n",
    "        self.num_words_in_longest_question = max([self.data_and_metadata[i][3] for i in range(3)])\n",
    "        self.num_sentences_in_longest_input = max([self.data_and_metadata[i][4] for i in range(3)])\n",
    "        \n",
    "        self.embedded_data = [self.embed_and_pad_data(datatype_id = i) for i in range(4)]\n",
    "        \n",
    "        self.create_position_encoding()\n",
    "        \n",
    "    def answer_user_data(self, inputs, questions):    \n",
    "        # load model on user data\n",
    "        self.data_and_metadata[self.datatype_id_dict['user']] = self.preprocess_user_data(inputs, questions)\n",
    "        self.embedded_data[self.datatype_id_dict['user']] = self.embed_and_pad_data(self.datatype_id_dict['user'])\n",
    "        # get predictions\n",
    "        predictions = self.sess.run(self.predictions, feed_dict = self.get_batch(datatype = 'user', batch_number = 0))\n",
    "        predictions = [self.id_word_dict[id] for id in predictions]\n",
    "        return(predictions)\n",
    "    \n",
    "    def preprocess_user_data(self, inputs = [], questions = []):    \n",
    "        num_sentences_in_each_chapter = []\n",
    "        chapter_input = []\n",
    "        data = []\n",
    "\n",
    "        for index in range(len(inputs)):\n",
    "            input = re.sub('[?]', '', inputs[index]).lower().split('.')\n",
    "            chapter_input = []\n",
    "            for sentence in input:\n",
    "                if sentence != '':\n",
    "                    chapter_input.append(sentence.split())\n",
    "            cleaned_question = re.sub('[?]', '', questions[index]).lower().split()\n",
    "            data.append({'I': chapter_input,\n",
    "                         'Q': cleaned_question,\n",
    "                         'A': [None]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "        num_chapters = len(data)\n",
    "        \n",
    "        return([data, num_sentences_in_each_chapter, None,\n",
    "              None, None, num_chapters])\n",
    "        \n",
    "    def get_batch(self, datatype, batch_number):\n",
    "        index = self.datatype_id_dict[datatype]\n",
    "        return {self.inputs: self.embedded_data[index][0][batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.questions: self.embedded_data[index][1][batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.answers: self.embedded_data[index][2][batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.input_lengths: self.data_and_metadata[index][1][batch_number*self.batch_size: (batch_number+1)*self.batch_size]\n",
    "               }\n",
    "    \n",
    "    def perform_epoch(self, num_chapters, datatype):\n",
    "        epoch_loss = epoch_num_correct = 0\n",
    "        for batch_idx in range(num_chapters/self.batch_size):\n",
    "            batch_loss, batch_num_correct, _ = self.sess.run((self.loss, self.num_correct, (self.optimizer if datatype == 'train' else self.question_vector)), \n",
    "                                                                        feed_dict = self.get_batch(datatype = datatype, batch_number = batch_idx))\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_num_correct += batch_num_correct\n",
    "        return(epoch_loss, epoch_num_correct)\n",
    "                \n",
    "    def train(self):            \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        best_valid_loss = float(\"inf\")\n",
    "        previous_valid_loss = float(\"inf\")\n",
    "        is_valid_loss_greater_strip = []\n",
    "        train_num_chapters = self.data_and_metadata[self.datatype_id_dict['train']][5]\n",
    "        valid_num_chapters = self.data_and_metadata[self.datatype_id_dict['valid']][5]\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss, epoch_num_correct = self.perform_epoch(train_num_chapters, 'train')\n",
    "            print(\"Epoch %d: %.2f%% complete, %d mins, Avg loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss/train_num_chapters, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/train_num_chapters))\n",
    "            # early stopping\n",
    "            if epoch%self.num_epochs_before_checking_valid_loss == 0:\n",
    "                epoch_loss, epoch_num_correct = self.perform_epoch(valid_num_chapters, 'valid')\n",
    "                print(\"\\nValidation avg loss: %.2f, Num correct: %d, Accuracy: %.2f%%\"%(epoch_loss/valid_num_chapters, epoch_num_correct, float(epoch_num_correct*100)/valid_num_chapters))\n",
    "                # self.save()\n",
    "                \n",
    "                # UP stopping criterion (from http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)\n",
    "                is_valid_loss_greater_strip.append(False if epoch_loss < previous_valid_loss else True)\n",
    "                if(sum(is_valid_loss_greater_strip[-self.num_consecutive_strips_before_stopping:]) == self.num_consecutive_strips_before_stopping):\n",
    "                    print(\"Stopping Early\\nDuration: %d mins\" % int((time.time() - start_time)/60))\n",
    "                    return\n",
    "                \n",
    "                if epoch_loss < best_valid_loss:\n",
    "                    best_valid_loss = epoch_loss\n",
    "                    self.save()                \n",
    "                previous_valid_loss = epoch_loss\n",
    "                # crude stopping criterion (stop as soon as validation loss increases)\n",
    "                # if new validation loss is lower than the old one, save new weights\n",
    "#                 if epoch_loss < old_valid_loss:\n",
    "#                     old_valid_loss = epoch_loss\n",
    "#                     self.save()\n",
    "#                 # else, stop training\n",
    "#                 else:\n",
    "#                     print(\"Stopping Early\\nDuration: %d mins\" % int((time.time() - start_time)/60))\n",
    "#                     return\n",
    "        print(\"Duration: %d mins\" % int((time.time() - start_time)/60))\n",
    "        \n",
    "    def save(self):\n",
    "        # create filename based on task\n",
    "        save_path = self.saver.save(self.sess, \"../saved-models/task-%d.ckpt\"%self.task)\n",
    "        print(\"Model saved in file: %s\\n\" % save_path)\n",
    "        \n",
    "    def restore(self, task):\n",
    "        # check if there is any saved model for that particular task\n",
    "        if os.path.isfile(\"../saved-models/task-%d.ckpt.meta\"%task):\n",
    "            self.saver.restore(self.sess, \"../saved-models/task-%d.ckpt\"%task)\n",
    "            print(\"Model restored\")\n",
    "        else:\n",
    "            print(\"Saved model for given task does not exist\")\n",
    "        \n",
    "    def test(self):\n",
    "        # load the weights from the model that generated the best validation loss\n",
    "        self.restore(self.task)\n",
    "        start_time = time.time()\n",
    "        num_chapters = self.data_and_metadata[self.datatype_id_dict['test']][5]\n",
    "        total_loss, total_num_correct = self.perform_epoch(num_chapters, 'test')\n",
    "        print(\"%d mins, Avg loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % ((time.time() - start_time)/60,\n",
    "                                                                          total_loss/num_chapters,\n",
    "                                                                          total_num_correct,\n",
    "                                                                          total_num_correct*100.0/num_chapters))\n",
    "        \n",
    "    def create_tensorflow_graph(self):\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, self.num_sentences_in_longest_input, self.num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        self.questions = tf.placeholder(tf.float32, [None, None, self.embedding_dim])\n",
    "        self.answers = tf.placeholder(tf.int32, [None])\n",
    "        self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        ## Question module\n",
    "        with tf.variable_scope('question_module'):\n",
    "            question_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            _, self.question_vector = tf.nn.dynamic_rnn(question_gru_cell,\n",
    "                                                  self.questions,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "        ## Input module\n",
    "        with tf.variable_scope('input_module'):\n",
    "\n",
    "            positionally_encoded_inputs = tf.reduce_sum(self.inputs*self.position_encoding, 2)\n",
    "\n",
    "            input_forward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_backward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_module_output, _ = tf.nn.bidirectional_dynamic_rnn(input_forward_gru_cell,\n",
    "                                                                    input_backward_gru_cell,\n",
    "                                                                    positionally_encoded_inputs,\n",
    "                                                                    sequence_length = self.input_lengths,\n",
    "                                                                    dtype = tf.float32)\n",
    "            input_fact_vectors = tf.add(input_module_output[0], input_module_output[1])\n",
    "            input_fact_vectors = tf.nn.dropout(input_fact_vectors, self.dropout_probability)\n",
    "\n",
    "        ## Episodic Memory module\n",
    "        with tf.variable_scope('episodic_memory_module'):\n",
    "            weight = tf.get_variable(\"weight\", [3*self.hidden_layer_size, 80],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            bias = tf.get_variable(\"bias\", [1, self.hidden_layer_size],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            self.previous_memory = self.question_vector\n",
    "            for step in range(self.num_steps):\n",
    "                attentions = []\n",
    "                for fact_index, fact_vector in enumerate(tf.unstack(input_fact_vectors, axis = 1)):\n",
    "                    reuse = bool(step) or bool(fact_index)\n",
    "                    with tf.variable_scope(\"attention\", reuse = reuse):\n",
    "                        z = tf.concat([tf.multiply(fact_vector, self.question_vector), \n",
    "                                       tf.multiply(fact_vector, self.previous_memory),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.question_vector)),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.previous_memory))], 1)\n",
    "                        attention = tf.contrib.layers.fully_connected(z,\n",
    "                                                                    self.embedding_dim,\n",
    "                                                                    activation_fn=tf.nn.tanh,\n",
    "                                                                    reuse=reuse, scope=\"fc1\")\n",
    "                        attention = tf.contrib.layers.fully_connected(attention,\n",
    "                                                                    1,\n",
    "                                                                    activation_fn=None,\n",
    "                                                                    reuse=reuse, scope=\"fc2\")\n",
    "                        attentions.append(tf.squeeze(attention))\n",
    "                attentions = tf.expand_dims(tf.nn.softmax(tf.transpose(tf.stack(attentions))), axis=-1)\n",
    "                reuse = True if step > 0 else False\n",
    "                # soft attention\n",
    "                self.context_vector = tf.reduce_sum(tf.multiply(input_fact_vectors, attentions), axis = 1)\n",
    "                self.previous_memory = tf.nn.relu(tf.matmul(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1), \n",
    "                                                            weight) + bias)\n",
    "                        \n",
    "            self.previous_memory = tf.nn.dropout(self.previous_memory, self.dropout_probability)\n",
    "\n",
    "        ## Answer module\n",
    "        with tf.variable_scope('answer_module') as scope:\n",
    "            logits = tf.contrib.layers.fully_connected(inputs = tf.concat([self.previous_memory, self.question_vector], axis = 1),\n",
    "                                                      num_outputs = self.vocab_size,\n",
    "                                                      activation_fn = None)\n",
    "\n",
    "            ## Loss and metrics\n",
    "            self.loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = self.answers))\n",
    "\n",
    "            # add l2 regularization for all variables except biases\n",
    "            for v in tf.trainable_variables():\n",
    "                if not 'bias' in v.name.lower():\n",
    "                    self.loss += self.l2_regularization_lambda * tf.nn.l2_loss(v)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            self.predictions = tf.cast(tf.argmax(tf.nn.softmax(logits), 1), 'int32')\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(tf.equal(self.predictions, self.answers), tf.int32))\n",
    "            \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.00% complete, 0 mins, Avg loss: 2.24, Num correct: 2344, Accuracy: 26.04%\n",
      "\n",
      "Validation avg loss: 1.66, Num correct: 379, Accuracy: 37.90%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 1: 0.78% complete, 0 mins, Avg loss: 1.49, Num correct: 4114, Accuracy: 45.71%\n",
      "Epoch 2: 1.56% complete, 0 mins, Avg loss: 1.06, Num correct: 6037, Accuracy: 67.08%\n",
      "Epoch 3: 2.34% complete, 0 mins, Avg loss: 0.48, Num correct: 7934, Accuracy: 88.16%\n",
      "Epoch 4: 3.12% complete, 0 mins, Avg loss: 0.24, Num correct: 8660, Accuracy: 96.22%\n",
      "Epoch 5: 3.91% complete, 1 mins, Avg loss: 0.17, Num correct: 8835, Accuracy: 98.17%\n",
      "\n",
      "Validation avg loss: 0.12, Num correct: 992, Accuracy: 99.20%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 6: 4.69% complete, 1 mins, Avg loss: 0.14, Num correct: 8907, Accuracy: 98.97%\n",
      "Epoch 7: 5.47% complete, 1 mins, Avg loss: 0.13, Num correct: 8931, Accuracy: 99.23%\n",
      "Epoch 8: 6.25% complete, 1 mins, Avg loss: 0.13, Num correct: 8916, Accuracy: 99.07%\n",
      "Epoch 9: 7.03% complete, 1 mins, Avg loss: 0.12, Num correct: 8961, Accuracy: 99.57%\n",
      "Epoch 10: 7.81% complete, 1 mins, Avg loss: 0.12, Num correct: 8963, Accuracy: 99.59%\n",
      "\n",
      "Validation avg loss: 0.11, Num correct: 995, Accuracy: 99.50%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 11: 8.59% complete, 2 mins, Avg loss: 0.12, Num correct: 8965, Accuracy: 99.61%\n",
      "Epoch 12: 9.38% complete, 2 mins, Avg loss: 0.11, Num correct: 8974, Accuracy: 99.71%\n",
      "Epoch 13: 10.16% complete, 2 mins, Avg loss: 0.10, Num correct: 8988, Accuracy: 99.87%\n",
      "Epoch 14: 10.94% complete, 2 mins, Avg loss: 0.12, Num correct: 8969, Accuracy: 99.66%\n",
      "Epoch 15: 11.72% complete, 2 mins, Avg loss: 0.12, Num correct: 8963, Accuracy: 99.59%\n",
      "\n",
      "Validation avg loss: 0.11, Num correct: 998, Accuracy: 99.80%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 16: 12.50% complete, 3 mins, Avg loss: 0.11, Num correct: 8976, Accuracy: 99.73%\n",
      "Epoch 17: 13.28% complete, 3 mins, Avg loss: 0.10, Num correct: 8982, Accuracy: 99.80%\n",
      "Epoch 18: 14.06% complete, 3 mins, Avg loss: 0.10, Num correct: 8986, Accuracy: 99.84%\n",
      "Epoch 19: 14.84% complete, 3 mins, Avg loss: 0.11, Num correct: 8973, Accuracy: 99.70%\n",
      "Epoch 20: 15.62% complete, 3 mins, Avg loss: 0.10, Num correct: 8975, Accuracy: 99.72%\n",
      "\n",
      "Validation avg loss: 0.11, Num correct: 997, Accuracy: 99.70%\n",
      "Epoch 21: 16.41% complete, 3 mins, Avg loss: 0.11, Num correct: 8951, Accuracy: 99.46%\n",
      "Epoch 22: 17.19% complete, 4 mins, Avg loss: 0.13, Num correct: 8949, Accuracy: 99.43%\n",
      "Epoch 23: 17.97% complete, 4 mins, Avg loss: 0.11, Num correct: 8982, Accuracy: 99.80%\n",
      "Epoch 24: 18.75% complete, 4 mins, Avg loss: 0.10, Num correct: 8987, Accuracy: 99.86%\n",
      "Epoch 25: 19.53% complete, 4 mins, Avg loss: 0.11, Num correct: 8965, Accuracy: 99.61%\n",
      "\n",
      "Validation avg loss: 0.10, Num correct: 999, Accuracy: 99.90%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 26: 20.31% complete, 4 mins, Avg loss: 0.10, Num correct: 8981, Accuracy: 99.79%\n",
      "Epoch 27: 21.09% complete, 5 mins, Avg loss: 0.10, Num correct: 8992, Accuracy: 99.91%\n",
      "Epoch 28: 21.88% complete, 5 mins, Avg loss: 0.09, Num correct: 8991, Accuracy: 99.90%\n",
      "Epoch 29: 22.66% complete, 5 mins, Avg loss: 0.10, Num correct: 8983, Accuracy: 99.81%\n",
      "Epoch 30: 23.44% complete, 5 mins, Avg loss: 0.10, Num correct: 8987, Accuracy: 99.86%\n",
      "\n",
      "Validation avg loss: 0.11, Num correct: 998, Accuracy: 99.80%\n",
      "Epoch 31: 24.22% complete, 5 mins, Avg loss: 0.11, Num correct: 8969, Accuracy: 99.66%\n",
      "Epoch 32: 25.00% complete, 5 mins, Avg loss: 0.10, Num correct: 8986, Accuracy: 99.84%\n",
      "Epoch 33: 25.78% complete, 6 mins, Avg loss: 0.10, Num correct: 8982, Accuracy: 99.80%\n",
      "Epoch 34: 26.56% complete, 6 mins, Avg loss: 0.11, Num correct: 8975, Accuracy: 99.72%\n",
      "Epoch 35: 27.34% complete, 6 mins, Avg loss: 0.10, Num correct: 8974, Accuracy: 99.71%\n",
      "\n",
      "Validation avg loss: 0.10, Num correct: 996, Accuracy: 99.60%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 36: 28.12% complete, 6 mins, Avg loss: 0.10, Num correct: 8977, Accuracy: 99.74%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f7cb7075e98e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdmn_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-026767bf449b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num_chapters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             print(\"Epoch %d: %.2f%% complete, %d mins, Avg loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n\u001b[1;32m    181\u001b[0m                                                                                    \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-026767bf449b>\u001b[0m in \u001b[0;36mperform_epoch\u001b[0;34m(self, num_chapters, datatype)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_chapters\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             batch_loss, batch_num_correct, _ = self.sess.run((self.loss, self.num_correct, (self.optimizer if datatype == 'train' else self.question_vector)), \n\u001b[0;32m--> 165\u001b[0;31m                                                                         feed_dict = self.get_batch(datatype = datatype, batch_number = batch_idx))\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mepoch_num_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_num_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = dmn_plus(task = 1)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../saved-models/task-3.ckpt\n",
      "Model restored\n",
      "0 mins, Avg loss: 2.11, Num correct: 207, Accuracy: 20.70%\n"
     ]
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get word answer for input-question pairs (in word form) for GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.answer_user_data(inputs = [\"John went to the bathroom. Adam went to the office. John went back to the garden.\"], questions = [\"Where is John?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = dmn_plus(task = 2)\n",
    "# model.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
