{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "\n",
    "def load_babi_data(is_training, task, embedding_dim, embedding, word_id_dict):\n",
    "    if is_training:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\"\n",
    "    else:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_test.txt\"\n",
    "    \n",
    "    file = open(filepath)\n",
    "    num_words_in_longest_input_sentence = 0\n",
    "    num_words_in_longest_question = 0\n",
    "    num_sentences_in_each_chapter = []\n",
    "    chapter_input = []\n",
    "    data = []\n",
    "    \n",
    "    for line in file:\n",
    "        items = re.sub('[?.]', '', line).lower().split()\n",
    "        if items[-1].isdigit():\n",
    "            data.append({'I': copy.deepcopy(chapter_input),\n",
    "                     'Q': items[1:-2],\n",
    "                     'A': [items[-2]]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "            num_words_in_longest_question = max(num_words_in_longest_question, len(items[1:-2]))\n",
    "        else:\n",
    "            if items[0] == '1':\n",
    "                chapter_input = [items[1:]]\n",
    "            else:\n",
    "                chapter_input.append(items[1:])\n",
    "            num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, len(items[1:]))\n",
    "    file.close()\n",
    "\n",
    "    num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "    num_chapters = len(data)\n",
    "\n",
    "    data_inputs = np.zeros([num_chapters, num_sentences_in_longest_input, num_words_in_longest_input_sentence, embedding_dim])\n",
    "    data_questions = np.zeros([num_chapters, num_words_in_longest_question, embedding_dim])\n",
    "    data_answers = np.zeros([num_chapters])\n",
    "    for chapter_index, chapter in enumerate(data):\n",
    "        for sentence_index, sentence in enumerate(chapter['I']):\n",
    "            data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = embedding[[word_id_dict[word] for word in sentence]]\n",
    "        data_questions[chapter_index, 0:len(chapter['Q']), :] = embedding[[word_id_dict[word] for word in chapter['Q']]]\n",
    "        data_answers[chapter_index] = word_id_dict[chapter['A'][0]]\n",
    "    \n",
    "    return(data_inputs, data_questions, data_answers, \n",
    "           num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "           num_words_in_longest_question, num_sentences_in_longest_input, \n",
    "           num_chapters)\n",
    "\n",
    "def load_glove():\n",
    "    vocab_size = 400000\n",
    "    embedding_dim = 50\n",
    "    file = open(\"../../../datasets/glove_6b/glove.6B.50d.txt\")    \n",
    "    embedding = np.ndarray([vocab_size, embedding_dim])\n",
    "    word_id_dict = {}\n",
    "    id_word_dict = {}\n",
    "    id = 0\n",
    "    for line in file:\n",
    "        items = line.split(' ')\n",
    "        word_id_dict[items[0]] = id\n",
    "        id_word_dict[id] = items[0]\n",
    "        embedding[id,:] = np.array([float(i) for i in items[1:]])\n",
    "        id += 1\n",
    "    file.close()\n",
    "    return(embedding, word_id_dict, id_word_dict, vocab_size, embedding_dim)\n",
    "\n",
    "def create_position_encoding(embedding_dim, num_words_in_longest_input_sentence):\n",
    "    ## Position encoding\n",
    "    position_encoding = np.ones([embedding_dim, num_words_in_longest_input_sentence], dtype=np.float32)\n",
    "\n",
    "    ## Below (my implementation, from section 3.1 in https://arxiv.org/pdf/1603.01417.pdf) didn't work.\n",
    "    # for j in range(1, num_words_in_longest_input_sentence+1):\n",
    "    #     for d in range(1, embedding_dim+1):\n",
    "    #         position_encoding[d-1, j-1] = (1 - j/num_words_in_longest_input_sentence) - (d/embedding_dim)*(1 - 2*j/num_words_in_longest_input_sentence)\n",
    "\n",
    "    ## Copied from https://github.com/domluna/memn2n\n",
    "    ls = num_words_in_longest_input_sentence+1\n",
    "    le = embedding_dim+1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            position_encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)\n",
    "    position_encoding = 1 + 4 * position_encoding / embedding_dim / num_words_in_longest_input_sentence\n",
    "    position_encoding = np.transpose(position_encoding)\n",
    "    return(position_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dmn_plus:\n",
    "    def __init__(self):\n",
    "        self.hidden_layer_size = 80\n",
    "        self.num_steps = 3\n",
    "        self.batch_size = 100\n",
    "        self.dropout_probability = 0.9\n",
    "        self.l2_regularization_lambda = 0.001\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 100\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        self.embedding, self.word_id_dict, self.id_word_dict, self.vocab_size, self.embedding_dim = load_glove()\n",
    "        \n",
    "    def load_data(self, is_training = True, task = 1):\n",
    "        self.data_inputs, self.data_questions, self.data_answers, self.num_sentences_in_each_chapter, self.num_words_in_longest_input_sentence, self.num_words_in_longest_question, self.num_sentences_in_longest_input, self.num_chapters = load_babi_data(is_training, task, self.embedding_dim, self.embedding, self.word_id_dict)\n",
    "        self.position_encoding = create_position_encoding(self.embedding_dim, self.num_words_in_longest_input_sentence)\n",
    "\n",
    "    def get_batch(self, batch_number):\n",
    "        return {self.inputs: self.data_inputs[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.questions: self.data_questions[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.answers: self.data_answers[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.input_lengths: self.num_sentences_in_each_chapter[batch_number*self.batch_size: (batch_number+1)*self.batch_size]\n",
    "               }\n",
    "\n",
    "    def train(self, sess):\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = epoch_num_correct = 0\n",
    "            for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "                batch_loss, batch_num_correct, _ = sess.run((self.loss, self.num_correct, self.optimizer), \n",
    "                                                                            feed_dict = self.get_batch(batch_idx))\n",
    "                epoch_loss += batch_loss\n",
    "                epoch_num_correct += batch_num_correct\n",
    "            print(\"Epoch %d: %.2f%% complete, %d mins, Loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/self.num_chapters))\n",
    "        end_time = time.time()\n",
    "        print(\"Duration: %d mins\" % int((end_time - start_time)/60))\n",
    "        \n",
    "    def test(self, sess):\n",
    "        start_time = time.time()\n",
    "        total_num_correct = 0\n",
    "        for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "            batch_num_correct = sess.run(self.num_correct, feed_dict = self.get_batch(batch_idx))\n",
    "            total_num_correct += batch_num_correct\n",
    "        print(\"%d mins, Num correct: %d, Accuracy: %.2f%%\" % ((time.time() - start_time)/60,\n",
    "                                                              total_num_correct,\n",
    "                                                              total_num_correct*100.0/self.num_chapters))\n",
    "        \n",
    "    def create_tensorflow_graph(self):\n",
    "        self.inputs = tf.placeholder(tf.float32, [self.batch_size, self.num_sentences_in_longest_input, self.num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        self.questions = tf.placeholder(tf.float32, [self.batch_size, self.num_words_in_longest_question, self.embedding_dim])\n",
    "        self.answers = tf.placeholder(tf.int32, [self.batch_size])\n",
    "        self.input_lengths = tf.placeholder(tf.int32, [self.batch_size])\n",
    "\n",
    "        ## Question module\n",
    "        with tf.variable_scope('question_module'):\n",
    "            question_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            _, question_vector = tf.nn.dynamic_rnn(question_gru_cell,\n",
    "                                                  self.questions,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "        ## Input module\n",
    "        with tf.variable_scope('input_module'):\n",
    "\n",
    "            positionally_encoded_inputs = tf.reduce_sum(self.inputs*self.position_encoding, 2)\n",
    "\n",
    "            input_forward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_backward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_module_output, _ = tf.nn.bidirectional_dynamic_rnn(input_forward_gru_cell,\n",
    "                                                                    input_backward_gru_cell,\n",
    "                                                                    positionally_encoded_inputs,\n",
    "                                                                    sequence_length = self.input_lengths,\n",
    "                                                                    dtype = tf.float32)\n",
    "            input_fact_vectors = tf.add(input_module_output[0], input_module_output[1])\n",
    "            input_fact_vectors = tf.nn.dropout(input_fact_vectors, self.dropout_probability)\n",
    "\n",
    "        ## Episodic Memory module\n",
    "        with tf.variable_scope('episodic_memory_module'):\n",
    "            previous_memory = question_vector\n",
    "            for step in range(self.num_steps):\n",
    "                attentions = []\n",
    "                for fact_index, fact_vector in enumerate(tf.unstack(input_fact_vectors, axis = 1)):\n",
    "                    reuse = bool(step) or bool(fact_index)\n",
    "                    with tf.variable_scope(\"attention\", reuse = reuse):\n",
    "                        z = tf.concat([tf.multiply(fact_vector, question_vector), \n",
    "                                       tf.multiply(fact_vector, previous_memory),\n",
    "                                       tf.abs(tf.subtract(fact_vector, question_vector)),\n",
    "                                       tf.abs(tf.subtract(fact_vector, previous_memory))], 1)\n",
    "                        attention = tf.contrib.layers.fully_connected(z,\n",
    "                                                                    self.embedding_dim,\n",
    "                                                                    activation_fn=tf.nn.tanh,\n",
    "                                                                    reuse=reuse, scope=\"fc1\")\n",
    "                        attention = tf.contrib.layers.fully_connected(attention,\n",
    "                                                                    1,\n",
    "                                                                    activation_fn=None,\n",
    "                                                                    reuse=reuse, scope=\"fc2\")\n",
    "                        attentions.append(tf.squeeze(attention))\n",
    "                attentions = tf.expand_dims(tf.nn.softmax(tf.transpose(tf.stack(attentions))), axis=-1)\n",
    "                reuse = True if step > 0 else False\n",
    "                # soft attention\n",
    "                context_vector = tf.reduce_sum(tf.multiply(input_fact_vectors, attentions), axis = 1)\n",
    "                with tf.variable_scope(\"step%d\"%step):\n",
    "                    previous_memory = tf.contrib.layers.fully_connected(tf.concat([previous_memory, context_vector, question_vector], axis = 1),\n",
    "                                                                                self.hidden_layer_size,\n",
    "                                                                                activation_fn=tf.nn.relu)\n",
    "            previous_memory = tf.nn.dropout(previous_memory, self.dropout_probability)\n",
    "\n",
    "        ## Answer module\n",
    "        with tf.variable_scope('answer_module') as scope:\n",
    "            logits = tf.contrib.layers.fully_connected(inputs = tf.concat([previous_memory, question_vector], axis = 1),\n",
    "                                                      num_outputs = self.vocab_size,\n",
    "                                                      activation_fn = None)\n",
    "\n",
    "            ## Loss and metrics\n",
    "            self.loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = self.answers))\n",
    "\n",
    "            # add l2 regularization for all variables except biases\n",
    "            for v in tf.trainable_variables():\n",
    "                if not 'bias' in v.name.lower():\n",
    "                    self.loss += self.l2_regularization_lambda * tf.nn.l2_loss(v)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            prediction = tf.cast(tf.argmax(tf.nn.softmax(logits), 1), 'int32')\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(tf.equal(prediction, self.answers), tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.00% complete, 0 mins, Loss: 12146.82, Num correct: 148, Accuracy: 14.80%\n",
      "Epoch 1: 1.00% complete, 0 mins, Loss: 5289.31, Num correct: 166, Accuracy: 16.60%\n",
      "Epoch 2: 2.00% complete, 0 mins, Loss: 1973.65, Num correct: 158, Accuracy: 15.80%\n",
      "Epoch 3: 3.00% complete, 0 mins, Loss: 2087.26, Num correct: 147, Accuracy: 14.70%\n",
      "Epoch 4: 4.00% complete, 0 mins, Loss: 2108.23, Num correct: 141, Accuracy: 14.10%\n",
      "Epoch 5: 5.00% complete, 0 mins, Loss: 1990.63, Num correct: 151, Accuracy: 15.10%\n",
      "Epoch 6: 6.00% complete, 0 mins, Loss: 1901.46, Num correct: 150, Accuracy: 15.00%\n",
      "Epoch 7: 7.00% complete, 0 mins, Loss: 1906.38, Num correct: 139, Accuracy: 13.90%\n",
      "Epoch 8: 8.00% complete, 0 mins, Loss: 1893.03, Num correct: 142, Accuracy: 14.20%\n",
      "Epoch 9: 9.00% complete, 0 mins, Loss: 1874.03, Num correct: 162, Accuracy: 16.20%\n",
      "Epoch 10: 10.00% complete, 0 mins, Loss: 1870.01, Num correct: 140, Accuracy: 14.00%\n",
      "Epoch 11: 11.00% complete, 0 mins, Loss: 1860.80, Num correct: 158, Accuracy: 15.80%\n",
      "Epoch 12: 12.00% complete, 0 mins, Loss: 1854.37, Num correct: 153, Accuracy: 15.30%\n",
      "Epoch 13: 13.00% complete, 0 mins, Loss: 1851.24, Num correct: 148, Accuracy: 14.80%\n",
      "Epoch 14: 14.00% complete, 0 mins, Loss: 1849.34, Num correct: 144, Accuracy: 14.40%\n",
      "Epoch 15: 15.00% complete, 0 mins, Loss: 1840.59, Num correct: 163, Accuracy: 16.30%\n",
      "Epoch 16: 16.00% complete, 0 mins, Loss: 1839.19, Num correct: 137, Accuracy: 13.70%\n",
      "Epoch 17: 17.00% complete, 0 mins, Loss: 1836.18, Num correct: 151, Accuracy: 15.10%\n",
      "Epoch 18: 18.00% complete, 0 mins, Loss: 1832.70, Num correct: 147, Accuracy: 14.70%\n",
      "Epoch 19: 19.00% complete, 0 mins, Loss: 1833.00, Num correct: 156, Accuracy: 15.60%\n",
      "Epoch 20: 20.00% complete, 0 mins, Loss: 1825.68, Num correct: 168, Accuracy: 16.80%\n",
      "Epoch 21: 21.00% complete, 0 mins, Loss: 1822.53, Num correct: 156, Accuracy: 15.60%\n",
      "Epoch 22: 22.00% complete, 0 mins, Loss: 1818.89, Num correct: 159, Accuracy: 15.90%\n",
      "Epoch 23: 23.00% complete, 0 mins, Loss: 1817.66, Num correct: 169, Accuracy: 16.90%\n",
      "Epoch 24: 24.00% complete, 0 mins, Loss: 1811.43, Num correct: 173, Accuracy: 17.30%\n",
      "Epoch 25: 25.00% complete, 0 mins, Loss: 1805.25, Num correct: 170, Accuracy: 17.00%\n",
      "Epoch 26: 26.00% complete, 0 mins, Loss: 1803.94, Num correct: 190, Accuracy: 19.00%\n",
      "Epoch 27: 27.00% complete, 0 mins, Loss: 1795.14, Num correct: 213, Accuracy: 21.30%\n",
      "Epoch 28: 28.00% complete, 0 mins, Loss: 1783.89, Num correct: 233, Accuracy: 23.30%\n",
      "Epoch 29: 29.00% complete, 0 mins, Loss: 1770.09, Num correct: 241, Accuracy: 24.10%\n",
      "Epoch 30: 30.00% complete, 0 mins, Loss: 1753.15, Num correct: 243, Accuracy: 24.30%\n",
      "Epoch 31: 31.00% complete, 0 mins, Loss: 1729.00, Num correct: 261, Accuracy: 26.10%\n",
      "Epoch 32: 32.00% complete, 0 mins, Loss: 1703.35, Num correct: 268, Accuracy: 26.80%\n",
      "Epoch 33: 33.00% complete, 0 mins, Loss: 1679.87, Num correct: 280, Accuracy: 28.00%\n",
      "Epoch 34: 34.00% complete, 0 mins, Loss: 1664.37, Num correct: 297, Accuracy: 29.70%\n",
      "Epoch 35: 35.00% complete, 0 mins, Loss: 1659.60, Num correct: 290, Accuracy: 29.00%\n",
      "Epoch 36: 36.00% complete, 0 mins, Loss: 1649.78, Num correct: 300, Accuracy: 30.00%\n",
      "Epoch 37: 37.00% complete, 0 mins, Loss: 1632.92, Num correct: 311, Accuracy: 31.10%\n",
      "Epoch 38: 38.00% complete, 0 mins, Loss: 1620.22, Num correct: 322, Accuracy: 32.20%\n",
      "Epoch 39: 39.00% complete, 0 mins, Loss: 1611.24, Num correct: 330, Accuracy: 33.00%\n",
      "Epoch 40: 40.00% complete, 0 mins, Loss: 1596.61, Num correct: 337, Accuracy: 33.70%\n",
      "Epoch 41: 41.00% complete, 0 mins, Loss: 1563.30, Num correct: 381, Accuracy: 38.10%\n",
      "Epoch 42: 42.00% complete, 0 mins, Loss: 1552.73, Num correct: 386, Accuracy: 38.60%\n",
      "Epoch 43: 43.00% complete, 0 mins, Loss: 1513.31, Num correct: 415, Accuracy: 41.50%\n",
      "Epoch 44: 44.00% complete, 0 mins, Loss: 1488.88, Num correct: 421, Accuracy: 42.10%\n",
      "Epoch 45: 45.00% complete, 0 mins, Loss: 1428.81, Num correct: 453, Accuracy: 45.30%\n",
      "Epoch 46: 46.00% complete, 0 mins, Loss: 1368.72, Num correct: 476, Accuracy: 47.60%\n",
      "Epoch 47: 47.00% complete, 0 mins, Loss: 1346.96, Num correct: 493, Accuracy: 49.30%\n",
      "Epoch 48: 48.00% complete, 0 mins, Loss: 1302.27, Num correct: 506, Accuracy: 50.60%\n",
      "Epoch 49: 49.00% complete, 0 mins, Loss: 1238.46, Num correct: 523, Accuracy: 52.30%\n",
      "Epoch 50: 50.00% complete, 0 mins, Loss: 1205.07, Num correct: 556, Accuracy: 55.60%\n",
      "Epoch 51: 51.00% complete, 1 mins, Loss: 1168.52, Num correct: 589, Accuracy: 58.90%\n",
      "Epoch 52: 52.00% complete, 1 mins, Loss: 1094.86, Num correct: 618, Accuracy: 61.80%\n",
      "Epoch 53: 53.00% complete, 1 mins, Loss: 927.28, Num correct: 701, Accuracy: 70.10%\n",
      "Epoch 54: 54.00% complete, 1 mins, Loss: 867.70, Num correct: 717, Accuracy: 71.70%\n",
      "Epoch 55: 55.00% complete, 1 mins, Loss: 677.96, Num correct: 800, Accuracy: 80.00%\n",
      "Epoch 56: 56.00% complete, 1 mins, Loss: 582.41, Num correct: 845, Accuracy: 84.50%\n",
      "Epoch 57: 57.00% complete, 1 mins, Loss: 483.02, Num correct: 875, Accuracy: 87.50%\n",
      "Epoch 58: 58.00% complete, 1 mins, Loss: 410.71, Num correct: 894, Accuracy: 89.40%\n",
      "Epoch 59: 59.00% complete, 1 mins, Loss: 381.17, Num correct: 917, Accuracy: 91.70%\n",
      "Epoch 60: 60.00% complete, 1 mins, Loss: 346.46, Num correct: 921, Accuracy: 92.10%\n",
      "Epoch 61: 61.00% complete, 1 mins, Loss: 301.81, Num correct: 923, Accuracy: 92.30%\n",
      "Epoch 62: 62.00% complete, 1 mins, Loss: 294.56, Num correct: 946, Accuracy: 94.60%\n",
      "Epoch 63: 63.00% complete, 1 mins, Loss: 179.28, Num correct: 957, Accuracy: 95.70%\n",
      "Epoch 64: 64.00% complete, 1 mins, Loss: 193.73, Num correct: 954, Accuracy: 95.40%\n",
      "Epoch 65: 65.00% complete, 1 mins, Loss: 136.66, Num correct: 969, Accuracy: 96.90%\n",
      "Epoch 66: 66.00% complete, 1 mins, Loss: 143.60, Num correct: 977, Accuracy: 97.70%\n",
      "Epoch 67: 67.00% complete, 1 mins, Loss: 136.98, Num correct: 968, Accuracy: 96.80%\n",
      "Epoch 68: 68.00% complete, 1 mins, Loss: 147.10, Num correct: 966, Accuracy: 96.60%\n",
      "Epoch 69: 69.00% complete, 1 mins, Loss: 182.05, Num correct: 956, Accuracy: 95.60%\n",
      "Epoch 70: 70.00% complete, 1 mins, Loss: 130.18, Num correct: 974, Accuracy: 97.40%\n",
      "Epoch 71: 71.00% complete, 1 mins, Loss: 98.95, Num correct: 979, Accuracy: 97.90%\n",
      "Epoch 72: 72.00% complete, 1 mins, Loss: 122.58, Num correct: 978, Accuracy: 97.80%\n",
      "Epoch 73: 73.00% complete, 1 mins, Loss: 117.23, Num correct: 977, Accuracy: 97.70%\n",
      "Epoch 74: 74.00% complete, 1 mins, Loss: 94.28, Num correct: 986, Accuracy: 98.60%\n",
      "Epoch 75: 75.00% complete, 1 mins, Loss: 76.43, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 76: 76.00% complete, 1 mins, Loss: 83.93, Num correct: 984, Accuracy: 98.40%\n",
      "Epoch 77: 77.00% complete, 1 mins, Loss: 80.52, Num correct: 983, Accuracy: 98.30%\n",
      "Epoch 78: 78.00% complete, 1 mins, Loss: 59.40, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 79: 79.00% complete, 1 mins, Loss: 67.90, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 80: 80.00% complete, 1 mins, Loss: 39.44, Num correct: 991, Accuracy: 99.10%\n",
      "Epoch 81: 81.00% complete, 1 mins, Loss: 76.55, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 82: 82.00% complete, 1 mins, Loss: 95.07, Num correct: 986, Accuracy: 98.60%\n",
      "Epoch 83: 83.00% complete, 1 mins, Loss: 58.48, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 84: 84.00% complete, 1 mins, Loss: 54.93, Num correct: 985, Accuracy: 98.50%\n",
      "Epoch 85: 85.00% complete, 1 mins, Loss: 75.68, Num correct: 986, Accuracy: 98.60%\n",
      "Epoch 86: 86.00% complete, 1 mins, Loss: 61.43, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 87: 87.00% complete, 1 mins, Loss: 65.21, Num correct: 991, Accuracy: 99.10%\n",
      "Epoch 88: 88.00% complete, 1 mins, Loss: 74.44, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 89: 89.00% complete, 1 mins, Loss: 72.96, Num correct: 985, Accuracy: 98.50%\n",
      "Epoch 90: 90.00% complete, 1 mins, Loss: 106.81, Num correct: 989, Accuracy: 98.90%\n",
      "Epoch 91: 91.00% complete, 1 mins, Loss: 61.44, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 92: 92.00% complete, 1 mins, Loss: 76.66, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 93: 93.00% complete, 1 mins, Loss: 106.53, Num correct: 976, Accuracy: 97.60%\n",
      "Epoch 94: 94.00% complete, 1 mins, Loss: 59.34, Num correct: 991, Accuracy: 99.10%\n",
      "Epoch 95: 95.00% complete, 1 mins, Loss: 43.37, Num correct: 993, Accuracy: 99.30%\n",
      "Epoch 96: 96.00% complete, 1 mins, Loss: 76.99, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 97: 97.00% complete, 1 mins, Loss: 33.24, Num correct: 995, Accuracy: 99.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: 98.00% complete, 1 mins, Loss: 25.55, Num correct: 998, Accuracy: 99.80%\n",
      "Epoch 99: 99.00% complete, 1 mins, Loss: 33.47, Num correct: 994, Accuracy: 99.40%\n",
      "Duration: 1 mins\n"
     ]
    }
   ],
   "source": [
    "model = dmn_plus()\n",
    "model.load_embeddings()\n",
    "model.load_data(is_training=True, task=1)\n",
    "model.create_tensorflow_graph()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mins, Num correct: 973, Accuracy: 97.30%\n"
     ]
    }
   ],
   "source": [
    "model.load_data(is_training=False, task=1)\n",
    "model.test(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
