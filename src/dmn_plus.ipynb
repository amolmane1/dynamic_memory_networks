{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import os.path\n",
    "\n",
    "def load_babi_data(is_training, task, embedding_dim, embedding, word_id_dict):\n",
    "    if is_training:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\"\n",
    "    else:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_test.txt\"\n",
    "    \n",
    "    file = open(filepath)\n",
    "    num_words_in_longest_input_sentence = 0\n",
    "    num_words_in_longest_question = 0\n",
    "    num_sentences_in_each_chapter = []\n",
    "    chapter_input = []\n",
    "    data = []\n",
    "    \n",
    "    for line in file:\n",
    "        items = re.sub('[?.]', '', line).lower().split()\n",
    "        if items[-1].isdigit():\n",
    "            data.append({'I': copy.deepcopy(chapter_input),\n",
    "                     'Q': items[1:-2],\n",
    "                     'A': [items[-2]]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "            num_words_in_longest_question = max(num_words_in_longest_question, len(items[1:-2]))\n",
    "        else:\n",
    "            if items[0] == '1':\n",
    "                chapter_input = [items[1:]]\n",
    "            else:\n",
    "                chapter_input.append(items[1:])\n",
    "            num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, len(items[1:]))\n",
    "    file.close()\n",
    "\n",
    "    num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "    num_chapters = len(data)\n",
    "\n",
    "    data_inputs = np.zeros([num_chapters, num_sentences_in_longest_input, num_words_in_longest_input_sentence, embedding_dim])\n",
    "    data_questions = np.zeros([num_chapters, num_words_in_longest_question, embedding_dim])\n",
    "    data_answers = np.zeros([num_chapters])\n",
    "    for chapter_index, chapter in enumerate(data):\n",
    "        for sentence_index, sentence in enumerate(chapter['I']):\n",
    "            data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = embedding[[word_id_dict[word] for word in sentence]]\n",
    "        data_questions[chapter_index, 0:len(chapter['Q']), :] = embedding[[word_id_dict[word] for word in chapter['Q']]]\n",
    "        data_answers[chapter_index] = word_id_dict[chapter['A'][0]]\n",
    "    \n",
    "    return(data_inputs, data_questions, data_answers, \n",
    "           num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "           num_words_in_longest_question, num_sentences_in_longest_input, \n",
    "           num_chapters)\n",
    "\n",
    "def load_glove():\n",
    "    vocab_size = 400000\n",
    "    embedding_dim = 50\n",
    "    file = open(\"../../../datasets/glove_6b/glove.6B.50d.txt\")    \n",
    "    embedding = np.ndarray([vocab_size, embedding_dim])\n",
    "    word_id_dict = {}\n",
    "    id_word_dict = {}\n",
    "    id = 0\n",
    "    for line in file:\n",
    "        items = line.split(' ')\n",
    "        word_id_dict[items[0]] = id\n",
    "        id_word_dict[id] = items[0]\n",
    "        embedding[id,:] = np.array([float(i) for i in items[1:]])\n",
    "        id += 1\n",
    "    file.close()\n",
    "    return(embedding, word_id_dict, id_word_dict, vocab_size, embedding_dim)\n",
    "\n",
    "def create_position_encoding(embedding_dim, num_words_in_longest_input_sentence):\n",
    "    ## Position encoding\n",
    "    position_encoding = np.ones([embedding_dim, num_words_in_longest_input_sentence], dtype=np.float32)\n",
    "\n",
    "    ## Below (my implementation, from section 3.1 in https://arxiv.org/pdf/1603.01417.pdf) didn't work.\n",
    "    # for j in range(1, num_words_in_longest_input_sentence+1):\n",
    "    #     for d in range(1, embedding_dim+1):\n",
    "    #         position_encoding[d-1, j-1] = (1 - j/num_words_in_longest_input_sentence) - (d/embedding_dim)*(1 - 2*j/num_words_in_longest_input_sentence)\n",
    "\n",
    "    ## Copied from https://github.com/domluna/memn2n\n",
    "    ls = num_words_in_longest_input_sentence+1\n",
    "    le = embedding_dim+1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            position_encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)\n",
    "    position_encoding = 1 + 4 * position_encoding / embedding_dim / num_words_in_longest_input_sentence\n",
    "    position_encoding = np.transpose(position_encoding)\n",
    "    return(position_encoding)\n",
    "\n",
    "\n",
    "class dmn_plus:\n",
    "    def __init__(self):\n",
    "        self.hidden_layer_size = 80\n",
    "        self.num_steps = 3\n",
    "        self.batch_size = 100\n",
    "        self.dropout_probability = 0.9\n",
    "        self.l2_regularization_lambda = 0.001\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 100\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        self.embedding, self.word_id_dict, self.id_word_dict, self.vocab_size, self.embedding_dim = load_glove()\n",
    "        \n",
    "    def load_data(self, is_training = True, task = 1):\n",
    "        self.task = task\n",
    "        self.data_inputs, self.data_questions, self.data_answers, self.num_sentences_in_each_chapter, self.num_words_in_longest_input_sentence, self.num_words_in_longest_question, self.num_sentences_in_longest_input, self.num_chapters = load_babi_data(is_training, self.task, self.embedding_dim, self.embedding, self.word_id_dict)\n",
    "        self.position_encoding = create_position_encoding(self.embedding_dim, self.num_words_in_longest_input_sentence)\n",
    "\n",
    "    def get_batch(self, batch_number):\n",
    "        return {self.inputs: self.data_inputs[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.questions: self.data_questions[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.answers: self.data_answers[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.input_lengths: self.num_sentences_in_each_chapter[batch_number*self.batch_size: (batch_number+1)*self.batch_size]\n",
    "               }\n",
    "\n",
    "    def train(self):            \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = epoch_num_correct = 0\n",
    "            for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "                batch_loss, batch_num_correct, _ = self.sess.run((self.loss, self.num_correct, self.optimizer), \n",
    "                                                                            feed_dict = self.get_batch(batch_idx))\n",
    "                epoch_loss += batch_loss\n",
    "                epoch_num_correct += batch_num_correct\n",
    "            print(\"Epoch %d: %.2f%% complete, %d mins, Loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/self.num_chapters))\n",
    "        end_time = time.time()\n",
    "        print(\"Duration: %d mins\" % int((end_time - start_time)/60))\n",
    "        \n",
    "    def save(self):\n",
    "        # create filename based on task\n",
    "        save_path = self.saver.save(self.sess, \"../saved-models/task-%d.ckpt\"%self.task)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "    def restore(self, task):\n",
    "        # check if there is any saved model for that particular task\n",
    "        if os.path.isfile(\"../saved-models/task-%d.ckpt.meta\"%task):\n",
    "            self.saver.restore(self.sess, \"../saved-models/task-%d.ckpt\"%task)\n",
    "            print(\"Model restored\")\n",
    "        else:\n",
    "            print(\"Saved model for given task does not exist\")\n",
    "        \n",
    "    def test(self):\n",
    "        start_time = time.time()\n",
    "        total_num_correct = 0\n",
    "        for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "            batch_num_correct = self.sess.run(self.num_correct, feed_dict = self.get_batch(batch_idx))\n",
    "            total_num_correct += batch_num_correct\n",
    "        print(\"%d mins, Num correct: %d, Accuracy: %.2f%%\" % ((time.time() - start_time)/60,\n",
    "                                                              total_num_correct,\n",
    "                                                              total_num_correct*100.0/self.num_chapters))\n",
    "        \n",
    "    def create_tensorflow_graph(self):\n",
    "        self.inputs = tf.placeholder(tf.float32, [self.batch_size, self.num_sentences_in_longest_input, self.num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        self.questions = tf.placeholder(tf.float32, [self.batch_size, self.num_words_in_longest_question, self.embedding_dim])\n",
    "        self.answers = tf.placeholder(tf.int32, [self.batch_size])\n",
    "        self.input_lengths = tf.placeholder(tf.int32, [self.batch_size])\n",
    "\n",
    "        ## Question module\n",
    "        with tf.variable_scope('question_module'):\n",
    "            question_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            _, self.question_vector = tf.nn.dynamic_rnn(question_gru_cell,\n",
    "                                                  self.questions,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "        ## Input module\n",
    "        with tf.variable_scope('input_module'):\n",
    "\n",
    "            positionally_encoded_inputs = tf.reduce_sum(self.inputs*self.position_encoding, 2)\n",
    "\n",
    "            input_forward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_backward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_module_output, _ = tf.nn.bidirectional_dynamic_rnn(input_forward_gru_cell,\n",
    "                                                                    input_backward_gru_cell,\n",
    "                                                                    positionally_encoded_inputs,\n",
    "                                                                    sequence_length = self.input_lengths,\n",
    "                                                                    dtype = tf.float32)\n",
    "            input_fact_vectors = tf.add(input_module_output[0], input_module_output[1])\n",
    "            input_fact_vectors = tf.nn.dropout(input_fact_vectors, self.dropout_probability)\n",
    "\n",
    "        ## Episodic Memory module\n",
    "        with tf.variable_scope('episodic_memory_module'):\n",
    "            self.previous_memory = self.question_vector\n",
    "            for step in range(self.num_steps):\n",
    "                attentions = []\n",
    "                for fact_index, fact_vector in enumerate(tf.unstack(input_fact_vectors, axis = 1)):\n",
    "                    reuse = bool(step) or bool(fact_index)\n",
    "                    with tf.variable_scope(\"attention\", reuse = reuse):\n",
    "                        z = tf.concat([tf.multiply(fact_vector, self.question_vector), \n",
    "                                       tf.multiply(fact_vector, self.previous_memory),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.question_vector)),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.previous_memory))], 1)\n",
    "                        attention = tf.contrib.layers.fully_connected(z,\n",
    "                                                                    self.embedding_dim,\n",
    "                                                                    activation_fn=tf.nn.tanh,\n",
    "                                                                    reuse=reuse, scope=\"fc1\")\n",
    "                        attention = tf.contrib.layers.fully_connected(attention,\n",
    "                                                                    1,\n",
    "                                                                    activation_fn=None,\n",
    "                                                                    reuse=reuse, scope=\"fc2\")\n",
    "                        attentions.append(tf.squeeze(attention))\n",
    "                attentions = tf.expand_dims(tf.nn.softmax(tf.transpose(tf.stack(attentions))), axis=-1)\n",
    "                reuse = True if step > 0 else False\n",
    "                # soft attention\n",
    "                self.context_vector = tf.reduce_sum(tf.multiply(input_fact_vectors, attentions), axis = 1)\n",
    "#                 self.trial = tf.expand_dims(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1), axis = -1)\n",
    "                with tf.variable_scope(\"step%d\"%step):\n",
    "                    self.previous_memory = tf.contrib.layers.fully_connected(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1),\n",
    "                                                                                self.hidden_layer_size,\n",
    "                                                                                activation_fn=tf.nn.relu)\n",
    "            self.previous_memory = tf.nn.dropout(self.previous_memory, self.dropout_probability)\n",
    "\n",
    "        ## Answer module\n",
    "        with tf.variable_scope('answer_module') as scope:\n",
    "            logits = tf.contrib.layers.fully_connected(inputs = tf.concat([self.previous_memory, self.question_vector], axis = 1),\n",
    "                                                      num_outputs = self.vocab_size,\n",
    "                                                      activation_fn = None)\n",
    "\n",
    "            ## Loss and metrics\n",
    "            self.loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = self.answers))\n",
    "\n",
    "            # add l2 regularization for all variables except biases\n",
    "            for v in tf.trainable_variables():\n",
    "                if not 'bias' in v.name.lower():\n",
    "                    self.loss += self.l2_regularization_lambda * tf.nn.l2_loss(v)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            prediction = tf.cast(tf.argmax(tf.nn.softmax(logits), 1), 'int32')\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(tf.equal(prediction, self.answers), tf.int32))\n",
    "            \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = dmn_plus()\n",
    "model.load_embeddings()\n",
    "model.load_data(is_training=True, task=1)\n",
    "model.create_tensorflow_graph()\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.00% complete, 0 mins, Loss: 12233.26, Num correct: 156, Accuracy: 15.60%\n",
      "Epoch 1: 1.00% complete, 0 mins, Loss: 5656.77, Num correct: 185, Accuracy: 18.50%\n",
      "Epoch 2: 2.00% complete, 0 mins, Loss: 1980.51, Num correct: 156, Accuracy: 15.60%\n",
      "Epoch 3: 3.00% complete, 0 mins, Loss: 2085.99, Num correct: 155, Accuracy: 15.50%\n",
      "Epoch 4: 4.00% complete, 0 mins, Loss: 2123.84, Num correct: 142, Accuracy: 14.20%\n",
      "Epoch 5: 5.00% complete, 0 mins, Loss: 2013.23, Num correct: 147, Accuracy: 14.70%\n",
      "Epoch 6: 6.00% complete, 0 mins, Loss: 1901.00, Num correct: 159, Accuracy: 15.90%\n",
      "Epoch 7: 7.00% complete, 0 mins, Loss: 1901.42, Num correct: 154, Accuracy: 15.40%\n",
      "Epoch 8: 8.00% complete, 0 mins, Loss: 1891.14, Num correct: 154, Accuracy: 15.40%\n",
      "Epoch 9: 9.00% complete, 0 mins, Loss: 1869.91, Num correct: 150, Accuracy: 15.00%\n",
      "Epoch 10: 10.00% complete, 0 mins, Loss: 1869.10, Num correct: 120, Accuracy: 12.00%\n",
      "Epoch 11: 11.00% complete, 0 mins, Loss: 1861.34, Num correct: 141, Accuracy: 14.10%\n",
      "Epoch 12: 12.00% complete, 0 mins, Loss: 1852.74, Num correct: 148, Accuracy: 14.80%\n",
      "Epoch 13: 13.00% complete, 0 mins, Loss: 1848.87, Num correct: 151, Accuracy: 15.10%\n",
      "Epoch 14: 14.00% complete, 0 mins, Loss: 1843.60, Num correct: 153, Accuracy: 15.30%\n",
      "Epoch 15: 15.00% complete, 0 mins, Loss: 1841.62, Num correct: 164, Accuracy: 16.40%\n",
      "Epoch 16: 16.00% complete, 0 mins, Loss: 1839.34, Num correct: 150, Accuracy: 15.00%\n",
      "Epoch 17: 17.00% complete, 0 mins, Loss: 1830.73, Num correct: 160, Accuracy: 16.00%\n",
      "Epoch 18: 18.00% complete, 0 mins, Loss: 1827.91, Num correct: 164, Accuracy: 16.40%\n",
      "Epoch 19: 19.00% complete, 0 mins, Loss: 1821.14, Num correct: 170, Accuracy: 17.00%\n",
      "Epoch 20: 20.00% complete, 0 mins, Loss: 1821.75, Num correct: 156, Accuracy: 15.60%\n",
      "Epoch 21: 21.00% complete, 0 mins, Loss: 1818.69, Num correct: 172, Accuracy: 17.20%\n",
      "Epoch 22: 22.00% complete, 0 mins, Loss: 1815.60, Num correct: 167, Accuracy: 16.70%\n",
      "Epoch 23: 23.00% complete, 0 mins, Loss: 1808.11, Num correct: 183, Accuracy: 18.30%\n",
      "Epoch 24: 24.00% complete, 0 mins, Loss: 1800.15, Num correct: 195, Accuracy: 19.50%\n",
      "Epoch 25: 25.00% complete, 0 mins, Loss: 1784.31, Num correct: 257, Accuracy: 25.70%\n",
      "Epoch 26: 26.00% complete, 0 mins, Loss: 1769.23, Num correct: 277, Accuracy: 27.70%\n",
      "Epoch 27: 27.00% complete, 0 mins, Loss: 1736.81, Num correct: 327, Accuracy: 32.70%\n",
      "Epoch 28: 28.00% complete, 0 mins, Loss: 1684.76, Num correct: 352, Accuracy: 35.20%\n",
      "Epoch 29: 29.00% complete, 0 mins, Loss: 1643.28, Num correct: 332, Accuracy: 33.20%\n",
      "Epoch 30: 30.00% complete, 0 mins, Loss: 1596.11, Num correct: 359, Accuracy: 35.90%\n",
      "Epoch 31: 31.00% complete, 0 mins, Loss: 1559.19, Num correct: 361, Accuracy: 36.10%\n",
      "Epoch 32: 32.00% complete, 0 mins, Loss: 1535.21, Num correct: 369, Accuracy: 36.90%\n",
      "Epoch 33: 33.00% complete, 0 mins, Loss: 1524.18, Num correct: 366, Accuracy: 36.60%\n",
      "Epoch 34: 34.00% complete, 0 mins, Loss: 1493.18, Num correct: 371, Accuracy: 37.10%\n",
      "Epoch 35: 35.00% complete, 0 mins, Loss: 1483.20, Num correct: 376, Accuracy: 37.60%\n",
      "Epoch 36: 36.00% complete, 0 mins, Loss: 1459.55, Num correct: 391, Accuracy: 39.10%\n",
      "Epoch 37: 37.00% complete, 0 mins, Loss: 1464.17, Num correct: 391, Accuracy: 39.10%\n",
      "Epoch 38: 38.00% complete, 0 mins, Loss: 1435.33, Num correct: 400, Accuracy: 40.00%\n",
      "Epoch 39: 39.00% complete, 0 mins, Loss: 1403.46, Num correct: 398, Accuracy: 39.80%\n",
      "Epoch 40: 40.00% complete, 0 mins, Loss: 1388.09, Num correct: 410, Accuracy: 41.00%\n",
      "Epoch 41: 41.00% complete, 0 mins, Loss: 1340.76, Num correct: 433, Accuracy: 43.30%\n",
      "Epoch 42: 42.00% complete, 0 mins, Loss: 1289.93, Num correct: 472, Accuracy: 47.20%\n",
      "Epoch 43: 43.00% complete, 0 mins, Loss: 1250.25, Num correct: 484, Accuracy: 48.40%\n",
      "Epoch 44: 44.00% complete, 0 mins, Loss: 1199.21, Num correct: 523, Accuracy: 52.30%\n",
      "Epoch 45: 45.00% complete, 0 mins, Loss: 1124.01, Num correct: 556, Accuracy: 55.60%\n",
      "Epoch 46: 46.00% complete, 0 mins, Loss: 1118.97, Num correct: 541, Accuracy: 54.10%\n",
      "Epoch 47: 47.00% complete, 0 mins, Loss: 1055.18, Num correct: 585, Accuracy: 58.50%\n",
      "Epoch 48: 48.00% complete, 0 mins, Loss: 981.51, Num correct: 637, Accuracy: 63.70%\n",
      "Epoch 49: 49.00% complete, 0 mins, Loss: 811.95, Num correct: 704, Accuracy: 70.40%\n",
      "Epoch 50: 50.00% complete, 0 mins, Loss: 712.57, Num correct: 761, Accuracy: 76.10%\n",
      "Epoch 51: 51.00% complete, 1 mins, Loss: 601.64, Num correct: 800, Accuracy: 80.00%\n",
      "Epoch 52: 52.00% complete, 1 mins, Loss: 516.70, Num correct: 842, Accuracy: 84.20%\n",
      "Epoch 53: 53.00% complete, 1 mins, Loss: 441.94, Num correct: 868, Accuracy: 86.80%\n",
      "Epoch 54: 54.00% complete, 1 mins, Loss: 383.03, Num correct: 891, Accuracy: 89.10%\n",
      "Epoch 55: 55.00% complete, 1 mins, Loss: 321.71, Num correct: 916, Accuracy: 91.60%\n",
      "Epoch 56: 56.00% complete, 1 mins, Loss: 221.17, Num correct: 930, Accuracy: 93.00%\n",
      "Epoch 57: 57.00% complete, 1 mins, Loss: 194.83, Num correct: 960, Accuracy: 96.00%\n",
      "Epoch 58: 58.00% complete, 1 mins, Loss: 174.14, Num correct: 957, Accuracy: 95.70%\n",
      "Epoch 59: 59.00% complete, 1 mins, Loss: 190.55, Num correct: 955, Accuracy: 95.50%\n",
      "Epoch 60: 60.00% complete, 1 mins, Loss: 186.57, Num correct: 961, Accuracy: 96.10%\n",
      "Epoch 61: 61.00% complete, 1 mins, Loss: 128.33, Num correct: 969, Accuracy: 96.90%\n",
      "Epoch 62: 62.00% complete, 1 mins, Loss: 133.15, Num correct: 972, Accuracy: 97.20%\n",
      "Epoch 63: 63.00% complete, 1 mins, Loss: 99.98, Num correct: 983, Accuracy: 98.30%\n",
      "Epoch 64: 64.00% complete, 1 mins, Loss: 89.43, Num correct: 982, Accuracy: 98.20%\n",
      "Epoch 65: 65.00% complete, 1 mins, Loss: 102.00, Num correct: 978, Accuracy: 97.80%\n",
      "Epoch 66: 66.00% complete, 1 mins, Loss: 99.36, Num correct: 974, Accuracy: 97.40%\n",
      "Epoch 67: 67.00% complete, 1 mins, Loss: 71.72, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 68: 68.00% complete, 1 mins, Loss: 72.60, Num correct: 984, Accuracy: 98.40%\n",
      "Epoch 69: 69.00% complete, 1 mins, Loss: 51.83, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 70: 70.00% complete, 1 mins, Loss: 100.01, Num correct: 977, Accuracy: 97.70%\n",
      "Epoch 71: 71.00% complete, 1 mins, Loss: 66.51, Num correct: 983, Accuracy: 98.30%\n",
      "Epoch 72: 72.00% complete, 1 mins, Loss: 139.22, Num correct: 972, Accuracy: 97.20%\n",
      "Epoch 73: 73.00% complete, 1 mins, Loss: 96.22, Num correct: 982, Accuracy: 98.20%\n",
      "Epoch 74: 74.00% complete, 1 mins, Loss: 67.07, Num correct: 986, Accuracy: 98.60%\n",
      "Epoch 75: 75.00% complete, 1 mins, Loss: 64.39, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 76: 76.00% complete, 1 mins, Loss: 74.83, Num correct: 988, Accuracy: 98.80%\n",
      "Epoch 77: 77.00% complete, 1 mins, Loss: 57.07, Num correct: 987, Accuracy: 98.70%\n",
      "Epoch 78: 78.00% complete, 1 mins, Loss: 39.08, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 79: 79.00% complete, 1 mins, Loss: 60.13, Num correct: 985, Accuracy: 98.50%\n",
      "Epoch 80: 80.00% complete, 1 mins, Loss: 35.96, Num correct: 996, Accuracy: 99.60%\n",
      "Epoch 81: 81.00% complete, 1 mins, Loss: 40.86, Num correct: 995, Accuracy: 99.50%\n",
      "Epoch 82: 82.00% complete, 1 mins, Loss: 42.21, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 83: 83.00% complete, 1 mins, Loss: 41.30, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 84: 84.00% complete, 1 mins, Loss: 31.68, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 85: 85.00% complete, 1 mins, Loss: 56.43, Num correct: 990, Accuracy: 99.00%\n",
      "Epoch 86: 86.00% complete, 1 mins, Loss: 48.33, Num correct: 991, Accuracy: 99.10%\n",
      "Epoch 87: 87.00% complete, 1 mins, Loss: 67.70, Num correct: 989, Accuracy: 98.90%\n",
      "Epoch 88: 88.00% complete, 1 mins, Loss: 33.28, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 89: 89.00% complete, 1 mins, Loss: 47.48, Num correct: 995, Accuracy: 99.50%\n",
      "Epoch 90: 90.00% complete, 1 mins, Loss: 41.33, Num correct: 991, Accuracy: 99.10%\n",
      "Epoch 91: 91.00% complete, 1 mins, Loss: 28.18, Num correct: 997, Accuracy: 99.70%\n",
      "Epoch 92: 92.00% complete, 1 mins, Loss: 23.40, Num correct: 995, Accuracy: 99.50%\n",
      "Epoch 93: 93.00% complete, 1 mins, Loss: 38.06, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 94: 94.00% complete, 1 mins, Loss: 38.75, Num correct: 994, Accuracy: 99.40%\n",
      "Epoch 95: 95.00% complete, 1 mins, Loss: 39.13, Num correct: 991, Accuracy: 99.10%\n",
      "Epoch 96: 96.00% complete, 1 mins, Loss: 35.63, Num correct: 995, Accuracy: 99.50%\n",
      "Epoch 97: 97.00% complete, 1 mins, Loss: 29.51, Num correct: 996, Accuracy: 99.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: 98.00% complete, 1 mins, Loss: 33.30, Num correct: 992, Accuracy: 99.20%\n",
      "Epoch 99: 99.00% complete, 1 mins, Loss: 31.19, Num correct: 994, Accuracy: 99.40%\n",
      "Duration: 1 mins\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.sess.run(tf.global_variables_initializer())\n",
    "res = model.sess.run((model.trial), feed_dict = model.get_batch(0))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ../saved-models/task-1.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mins, Num correct: 990, Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_data(is_training=False, task=1)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../saved-models/task-1.ckpt\n",
      "Model restored\n",
      "0 mins, Num correct: 990, Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "model = dmn_plus()\n",
    "model.load_embeddings()\n",
    "model.load_data(is_training=False, task=1)\n",
    "model.create_tensorflow_graph()\n",
    "model.restore(task = 1)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get word answer for input-question pairs (in word form) for GUI\n",
    "#### NOTE: need to change tf code to allow for variable batch sizes before I can do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(self, input, question):\n",
    "    # convert inputs into embeddings\n",
    "    \n",
    "    # load model on inputs\n",
    "    \n",
    "    # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
