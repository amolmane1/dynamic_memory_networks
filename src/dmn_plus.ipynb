{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class dmn_plus:\n",
    "    def __init__(self, task):\n",
    "        self.vocab_size = 400000\n",
    "        self.embedding_dim = 50\n",
    "        self.hidden_layer_size = 80\n",
    "        self.num_steps = 3\n",
    "        self.batch_size = 100\n",
    "        self.dropout_probability = 0.9\n",
    "        self.l2_regularization_lambda = 0.001\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 128\n",
    "        self.num_epochs_before_checking_valid_loss = 5\n",
    "        self.num_consecutive_strips_before_stopping = 4\n",
    "        self.datatype_id_dict = {'train': 0, 'valid': 1, 'test': 2, 'user': 3}\n",
    "        self.datatypes = ['train', 'valid', 'test', 'user']\n",
    "        self.load_embeddings()\n",
    "        self.task = task\n",
    "        self.load_data(self.task)\n",
    "        self.create_tensorflow_graph()\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        file = open(\"../../../datasets/glove_6b/glove.6B.50d.txt\")    \n",
    "        self.embedding = np.ndarray([self.vocab_size, self.embedding_dim])\n",
    "        self.word_id_dict = {}\n",
    "        self.id_word_dict = {}\n",
    "        id = 0\n",
    "        for line in file:\n",
    "            items = line.split(' ')\n",
    "            self.word_id_dict[items[0]] = id\n",
    "            self.id_word_dict[id] = items[0]\n",
    "            self.embedding[id,:] = np.array([float(i) for i in items[1:]])\n",
    "            id += 1\n",
    "        file.close()\n",
    "        \n",
    "    def load_babi_data(self, datatype_id):\n",
    "        path_to_file_directory = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en-valid/\"\n",
    "        path_to_file = path_to_file_directory + 'qa' + str(self.task) + '_' + self.datatypes[datatype_id] + '.txt'\n",
    "        \n",
    "        file = open(path_to_file)\n",
    "        num_words_in_longest_input_sentence = 0\n",
    "        num_words_in_longest_question = 0\n",
    "        num_sentences_in_each_chapter = []\n",
    "        chapter_input = []\n",
    "        data = []\n",
    "\n",
    "        for line in file:\n",
    "            items = re.sub('[?.]', '', line).lower().split()\n",
    "            if items[-1].isdigit():\n",
    "                # find the index of the second digit in that line\n",
    "                index_of_second_digit = len(items)\n",
    "                for index in range(len(items)-1, 0, -1):\n",
    "                    if items[index].isdigit():\n",
    "                        index_of_second_digit = index\n",
    "                data.append({'I': copy.deepcopy(chapter_input),\n",
    "                         'Q': items[1:index_of_second_digit-1],\n",
    "                         'A': [items[index_of_second_digit-1]]})\n",
    "                num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "                num_words_in_longest_question = max(num_words_in_longest_question, len(items[1:index_of_second_digit-1]))\n",
    "            else:\n",
    "                if items[0] == '1':\n",
    "                    chapter_input = [items[1:]]\n",
    "                else:\n",
    "                    chapter_input.append(items[1:])\n",
    "                num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, len(items[1:]))\n",
    "        file.close()\n",
    "\n",
    "        num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "        num_chapters = len(data)\n",
    "\n",
    "        return([data, num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "              num_words_in_longest_question, num_sentences_in_longest_input, num_chapters])\n",
    "    \n",
    "    def embed_and_pad_data(self, datatype_id):\n",
    "        num_chapters = self.data_and_metadata[datatype_id][5]\n",
    "        data_inputs = np.zeros([num_chapters, self.num_sentences_in_longest_input, self.num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        data_questions = np.zeros([num_chapters, self.num_words_in_longest_question, self.embedding_dim])\n",
    "        data_answers = np.zeros([num_chapters])\n",
    "        for chapter_index, chapter in enumerate(self.data_and_metadata[datatype_id][0]):\n",
    "            for sentence_index, sentence in enumerate(chapter['I']):\n",
    "                data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = self.embedding[[self.word_id_dict[word] for word in sentence]]\n",
    "            data_questions[chapter_index, 0:len(chapter['Q']), :] = self.embedding[[self.word_id_dict[word] for word in chapter['Q']]]\n",
    "            data_answers[chapter_index] = None if chapter['A'][0] == None else self.word_id_dict[chapter['A'][0]]\n",
    "            \n",
    "        return([data_inputs, data_questions, data_answers])\n",
    "    \n",
    "    def create_position_encoding(self):\n",
    "        self.position_encoding = np.ones([self.embedding_dim, self.num_words_in_longest_input_sentence], dtype=np.float32)\n",
    "\n",
    "        ## Below (my implementation, from section 3.1 in https://arxiv.org/pdf/1603.01417.pdf) didn't work.\n",
    "        # for j in range(1, num_words_in_longest_input_sentence+1):\n",
    "        #     for d in range(1, embedding_dim+1):\n",
    "        #         position_encoding[d-1, j-1] = (1 - j/num_words_in_longest_input_sentence) - (d/embedding_dim)*(1 - 2*j/num_words_in_longest_input_sentence)\n",
    "\n",
    "        ## Copied from https://github.com/domluna/memn2n\n",
    "        ls = self.num_words_in_longest_input_sentence+1\n",
    "        le = self.embedding_dim+1\n",
    "        for i in range(1, le):\n",
    "            for j in range(1, ls):\n",
    "                self.position_encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)\n",
    "        self.position_encoding = 1 + 4 * self.position_encoding / self.embedding_dim / self.num_words_in_longest_input_sentence\n",
    "        self.position_encoding = np.transpose(self.position_encoding)\n",
    "        \n",
    "    def load_data(self, task):\n",
    "        self.data_and_metadata = [self.load_babi_data(datatype_id = i) for i in range(3)]      \n",
    "        self.data_and_metadata.append(self.preprocess_user_data())\n",
    "        \n",
    "        self.num_words_in_longest_input_sentence = max([self.data_and_metadata[i][2] for i in range(3)])\n",
    "        self.num_words_in_longest_question = max([self.data_and_metadata[i][3] for i in range(3)])\n",
    "        self.num_sentences_in_longest_input = max([self.data_and_metadata[i][4] for i in range(3)])\n",
    "        \n",
    "        self.embedded_data = [self.embed_and_pad_data(datatype_id = i) for i in range(4)]\n",
    "        \n",
    "        self.create_position_encoding()\n",
    "        \n",
    "    def answer_user_data(self, inputs, questions):    \n",
    "        # load model on user data\n",
    "        self.data_and_metadata[self.datatype_id_dict['user']] = self.preprocess_user_data(inputs, questions)\n",
    "        self.embedded_data[self.datatype_id_dict['user']] = self.embed_and_pad_data(self.datatype_id_dict['user'])\n",
    "        # get predictions\n",
    "        predictions = self.sess.run(self.predictions, feed_dict = self.get_batch(datatype = 'user', batch_number = 0))\n",
    "        predictions = [self.id_word_dict[id] for id in predictions]\n",
    "        return(predictions)\n",
    "    \n",
    "    def preprocess_user_data(self, inputs = [], questions = []):    \n",
    "        num_sentences_in_each_chapter = []\n",
    "        chapter_input = []\n",
    "        data = []\n",
    "\n",
    "        for index in range(len(inputs)):\n",
    "            input = re.sub('[?]', '', inputs[index]).lower().split('.')\n",
    "            chapter_input = []\n",
    "            for sentence in input:\n",
    "                if sentence != '':\n",
    "                    chapter_input.append(sentence.split())\n",
    "            cleaned_question = re.sub('[?]', '', questions[index]).lower().split()\n",
    "            data.append({'I': chapter_input,\n",
    "                         'Q': cleaned_question,\n",
    "                         'A': [None]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "        num_chapters = len(data)\n",
    "        \n",
    "        return([data, num_sentences_in_each_chapter, None,\n",
    "              None, None, num_chapters])\n",
    "        \n",
    "    def get_batch(self, datatype, batch_number):\n",
    "        index = self.datatype_id_dict[datatype]\n",
    "        return {self.inputs: self.embedded_data[index][0][batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.questions: self.embedded_data[index][1][batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.answers: self.embedded_data[index][2][batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.input_lengths: self.data_and_metadata[index][1][batch_number*self.batch_size: (batch_number+1)*self.batch_size]\n",
    "               }\n",
    "    \n",
    "    def perform_epoch(self, num_chapters, datatype):\n",
    "        epoch_loss = epoch_num_correct = 0\n",
    "        for batch_idx in range(num_chapters/self.batch_size):\n",
    "            batch_loss, batch_num_correct, _ = self.sess.run((self.loss, self.num_correct, (self.optimizer if datatype == 'train' else self.question_vector)), \n",
    "                                                                        feed_dict = self.get_batch(datatype = datatype, batch_number = batch_idx))\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_num_correct += batch_num_correct\n",
    "        return(epoch_loss, epoch_num_correct)\n",
    "                \n",
    "    def train(self):            \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        best_valid_loss = float(\"inf\")\n",
    "        previous_valid_loss = float(\"inf\")\n",
    "        is_valid_loss_greater_strip = []\n",
    "        train_num_chapters = self.data_and_metadata[self.datatype_id_dict['train']][5]\n",
    "        valid_num_chapters = self.data_and_metadata[self.datatype_id_dict['valid']][5]\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss, epoch_num_correct = self.perform_epoch(train_num_chapters, 'train')\n",
    "            print(\"Epoch %d: %.2f%% complete, %d mins, Avg loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss/train_num_chapters, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/train_num_chapters))\n",
    "            # early stopping\n",
    "            if epoch%self.num_epochs_before_checking_valid_loss == 0:\n",
    "                epoch_loss, epoch_num_correct = self.perform_epoch(valid_num_chapters, 'valid')\n",
    "                print(\"\\nValidation avg loss: %.2f, Num correct: %d, Accuracy: %.2f%%\"%(epoch_loss/valid_num_chapters, epoch_num_correct, float(epoch_num_correct*100)/valid_num_chapters))\n",
    "                # self.save()\n",
    "                \n",
    "                # UP stopping criterion (from http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)\n",
    "                is_valid_loss_greater_strip.append(False if epoch_loss < previous_valid_loss else True)\n",
    "                if(sum(is_valid_loss_greater_strip[-self.num_consecutive_strips_before_stopping:]) == self.num_consecutive_strips_before_stopping):\n",
    "                    print(\"Stopping Early\\nDuration: %d mins\" % int((time.time() - start_time)/60))\n",
    "                    return\n",
    "                \n",
    "                if epoch_loss < best_valid_loss:\n",
    "                    best_valid_loss = epoch_loss\n",
    "                    self.save()                \n",
    "                previous_valid_loss = epoch_loss\n",
    "                # crude stopping criterion (stop as soon as validation loss increases)\n",
    "                # if new validation loss is lower than the old one, save new weights\n",
    "#                 if epoch_loss < old_valid_loss:\n",
    "#                     old_valid_loss = epoch_loss\n",
    "#                     self.save()\n",
    "#                 # else, stop training\n",
    "#                 else:\n",
    "#                     print(\"Stopping Early\\nDuration: %d mins\" % int((time.time() - start_time)/60))\n",
    "#                     return\n",
    "        print(\"Duration: %d mins\" % int((time.time() - start_time)/60))\n",
    "        \n",
    "    def save(self):\n",
    "        # create filename based on task\n",
    "        save_path = self.saver.save(self.sess, \"../saved-models/task-%d.ckpt\"%self.task)\n",
    "        print(\"Model saved in file: %s\\n\" % save_path)\n",
    "        \n",
    "    def restore(self, task):\n",
    "        # check if there is any saved model for that particular task\n",
    "        if os.path.isfile(\"../saved-models/task-%d.ckpt.meta\"%task):\n",
    "            self.saver.restore(self.sess, \"../saved-models/task-%d.ckpt\"%task)\n",
    "            print(\"Model restored\")\n",
    "        else:\n",
    "            print(\"Saved model for given task does not exist\")\n",
    "        \n",
    "    def test(self):\n",
    "        # load the weights from the model that generated the best validation loss\n",
    "        self.restore(self.task)\n",
    "        start_time = time.time()\n",
    "        num_chapters = self.data_and_metadata[self.datatype_id_dict['test']][5]\n",
    "        total_loss, total_num_correct = self.perform_epoch(num_chapters, 'test')\n",
    "        print(\"%d mins, Avg loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % ((time.time() - start_time)/60,\n",
    "                                                                          total_loss/num_chapters,\n",
    "                                                                          total_num_correct,\n",
    "                                                                          total_num_correct*100.0/num_chapters))\n",
    "        \n",
    "    def create_tensorflow_graph(self):\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, self.num_sentences_in_longest_input, self.num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        self.questions = tf.placeholder(tf.float32, [None, None, self.embedding_dim])\n",
    "        self.answers = tf.placeholder(tf.int32, [None])\n",
    "        self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        ## Question module\n",
    "        with tf.variable_scope('question_module'):\n",
    "            question_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            _, self.question_vector = tf.nn.dynamic_rnn(question_gru_cell,\n",
    "                                                  self.questions,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "        ## Input module\n",
    "        with tf.variable_scope('input_module'):\n",
    "\n",
    "            positionally_encoded_inputs = tf.reduce_sum(self.inputs*self.position_encoding, 2)\n",
    "\n",
    "            input_forward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_backward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_module_output, _ = tf.nn.bidirectional_dynamic_rnn(input_forward_gru_cell,\n",
    "                                                                    input_backward_gru_cell,\n",
    "                                                                    positionally_encoded_inputs,\n",
    "                                                                    sequence_length = self.input_lengths,\n",
    "                                                                    dtype = tf.float32)\n",
    "            input_fact_vectors = tf.add(input_module_output[0], input_module_output[1])\n",
    "            input_fact_vectors = tf.nn.dropout(input_fact_vectors, self.dropout_probability)\n",
    "\n",
    "        ## Episodic Memory module\n",
    "        with tf.variable_scope('episodic_memory_module'):\n",
    "            weight = tf.get_variable(\"weight\", [3*self.hidden_layer_size, 80],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            bias = tf.get_variable(\"bias\", [1, self.hidden_layer_size],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            self.previous_memory = self.question_vector\n",
    "            for step in range(self.num_steps):\n",
    "                attentions = []\n",
    "                for fact_index, fact_vector in enumerate(tf.unstack(input_fact_vectors, axis = 1)):\n",
    "                    reuse = bool(step) or bool(fact_index)\n",
    "                    with tf.variable_scope(\"attention\", reuse = reuse):\n",
    "                        z = tf.concat([tf.multiply(fact_vector, self.question_vector), \n",
    "                                       tf.multiply(fact_vector, self.previous_memory),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.question_vector)),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.previous_memory))], 1)\n",
    "                        attention = tf.contrib.layers.fully_connected(z,\n",
    "                                                                    self.embedding_dim,\n",
    "                                                                    activation_fn=tf.nn.tanh,\n",
    "                                                                    reuse=reuse, scope=\"fc1\")\n",
    "                        attention = tf.contrib.layers.fully_connected(attention,\n",
    "                                                                    1,\n",
    "                                                                    activation_fn=None,\n",
    "                                                                    reuse=reuse, scope=\"fc2\")\n",
    "                        attentions.append(tf.squeeze(attention))\n",
    "                attentions = tf.expand_dims(tf.nn.softmax(tf.transpose(tf.stack(attentions))), axis=-1)\n",
    "                reuse = True if step > 0 else False\n",
    "                # soft attention\n",
    "                self.context_vector = tf.reduce_sum(tf.multiply(input_fact_vectors, attentions), axis = 1)\n",
    "                self.previous_memory = tf.nn.relu(tf.matmul(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1), \n",
    "                                                            weight) + bias)\n",
    "                        \n",
    "            self.previous_memory = tf.nn.dropout(self.previous_memory, self.dropout_probability)\n",
    "\n",
    "        ## Answer module\n",
    "        with tf.variable_scope('answer_module') as scope:\n",
    "            logits = tf.contrib.layers.fully_connected(inputs = tf.concat([self.previous_memory, self.question_vector], axis = 1),\n",
    "                                                      num_outputs = self.vocab_size,\n",
    "                                                      activation_fn = None)\n",
    "\n",
    "            ## Loss and metrics\n",
    "            self.loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = self.answers))\n",
    "\n",
    "            # add l2 regularization for all variables except biases\n",
    "            for v in tf.trainable_variables():\n",
    "                if not 'bias' in v.name.lower():\n",
    "                    self.loss += self.l2_regularization_lambda * tf.nn.l2_loss(v)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            self.predictions = tf.cast(tf.argmax(tf.nn.softmax(logits), 1), 'int32')\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(tf.equal(self.predictions, self.answers), tf.int32))\n",
    "            \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.00% complete, 0 mins, Avg loss: 4.75, Num correct: 111, Accuracy: 12.33%\n",
      "\n",
      "Validation avg loss: 2.62, Num correct: 16, Accuracy: 16.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 1: 0.78% complete, 0 mins, Avg loss: 2.11, Num correct: 182, Accuracy: 20.22%\n",
      "Epoch 2: 1.56% complete, 0 mins, Avg loss: 1.98, Num correct: 181, Accuracy: 20.11%\n",
      "Epoch 3: 2.34% complete, 0 mins, Avg loss: 1.87, Num correct: 205, Accuracy: 22.78%\n",
      "Epoch 4: 3.12% complete, 0 mins, Avg loss: 1.79, Num correct: 267, Accuracy: 29.67%\n",
      "Epoch 5: 3.91% complete, 0 mins, Avg loss: 1.71, Num correct: 307, Accuracy: 34.11%\n",
      "\n",
      "Validation avg loss: 1.70, Num correct: 40, Accuracy: 40.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 6: 4.69% complete, 0 mins, Avg loss: 1.62, Num correct: 363, Accuracy: 40.33%\n",
      "Epoch 7: 5.47% complete, 0 mins, Avg loss: 1.56, Num correct: 394, Accuracy: 43.78%\n",
      "Epoch 8: 6.25% complete, 0 mins, Avg loss: 1.54, Num correct: 394, Accuracy: 43.78%\n",
      "Epoch 9: 7.03% complete, 0 mins, Avg loss: 1.53, Num correct: 403, Accuracy: 44.78%\n",
      "Epoch 10: 7.81% complete, 0 mins, Avg loss: 1.46, Num correct: 432, Accuracy: 48.00%\n",
      "\n",
      "Validation avg loss: 1.61, Num correct: 41, Accuracy: 41.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 11: 8.59% complete, 0 mins, Avg loss: 1.40, Num correct: 450, Accuracy: 50.00%\n",
      "Epoch 12: 9.38% complete, 0 mins, Avg loss: 1.36, Num correct: 475, Accuracy: 52.78%\n",
      "Epoch 13: 10.16% complete, 0 mins, Avg loss: 1.37, Num correct: 467, Accuracy: 51.89%\n",
      "Epoch 14: 10.94% complete, 0 mins, Avg loss: 1.32, Num correct: 485, Accuracy: 53.89%\n",
      "Epoch 15: 11.72% complete, 0 mins, Avg loss: 1.28, Num correct: 498, Accuracy: 55.33%\n",
      "\n",
      "Validation avg loss: 1.40, Num correct: 48, Accuracy: 48.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 16: 12.50% complete, 0 mins, Avg loss: 1.25, Num correct: 524, Accuracy: 58.22%\n",
      "Epoch 17: 13.28% complete, 0 mins, Avg loss: 1.17, Num correct: 536, Accuracy: 59.56%\n",
      "Epoch 18: 14.06% complete, 0 mins, Avg loss: 1.03, Num correct: 594, Accuracy: 66.00%\n",
      "Epoch 19: 14.84% complete, 0 mins, Avg loss: 0.91, Num correct: 642, Accuracy: 71.33%\n",
      "Epoch 20: 15.62% complete, 0 mins, Avg loss: 0.88, Num correct: 644, Accuracy: 71.56%\n",
      "\n",
      "Validation avg loss: 1.07, Num correct: 69, Accuracy: 69.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 21: 16.41% complete, 0 mins, Avg loss: 0.73, Num correct: 718, Accuracy: 79.78%\n",
      "Epoch 22: 17.19% complete, 0 mins, Avg loss: 0.62, Num correct: 744, Accuracy: 82.67%\n",
      "Epoch 23: 17.97% complete, 0 mins, Avg loss: 0.58, Num correct: 759, Accuracy: 84.33%\n",
      "Epoch 24: 18.75% complete, 0 mins, Avg loss: 0.42, Num correct: 807, Accuracy: 89.67%\n",
      "Epoch 25: 19.53% complete, 0 mins, Avg loss: 0.41, Num correct: 810, Accuracy: 90.00%\n",
      "\n",
      "Validation avg loss: 0.54, Num correct: 86, Accuracy: 86.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 26: 20.31% complete, 0 mins, Avg loss: 0.37, Num correct: 817, Accuracy: 90.78%\n",
      "Epoch 27: 21.09% complete, 0 mins, Avg loss: 0.43, Num correct: 805, Accuracy: 89.44%\n",
      "Epoch 28: 21.88% complete, 0 mins, Avg loss: 0.34, Num correct: 833, Accuracy: 92.56%\n",
      "Epoch 29: 22.66% complete, 0 mins, Avg loss: 0.32, Num correct: 833, Accuracy: 92.56%\n",
      "Epoch 30: 23.44% complete, 0 mins, Avg loss: 0.28, Num correct: 851, Accuracy: 94.56%\n",
      "\n",
      "Validation avg loss: 0.34, Num correct: 95, Accuracy: 95.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 31: 24.22% complete, 0 mins, Avg loss: 0.25, Num correct: 855, Accuracy: 95.00%\n",
      "Epoch 32: 25.00% complete, 0 mins, Avg loss: 0.27, Num correct: 856, Accuracy: 95.11%\n",
      "Epoch 33: 25.78% complete, 0 mins, Avg loss: 0.28, Num correct: 853, Accuracy: 94.78%\n",
      "Epoch 34: 26.56% complete, 0 mins, Avg loss: 0.37, Num correct: 836, Accuracy: 92.89%\n",
      "Epoch 35: 27.34% complete, 0 mins, Avg loss: 0.29, Num correct: 850, Accuracy: 94.44%\n",
      "\n",
      "Validation avg loss: 0.35, Num correct: 92, Accuracy: 92.00%\n",
      "Epoch 36: 28.12% complete, 0 mins, Avg loss: 0.24, Num correct: 868, Accuracy: 96.44%\n",
      "Epoch 37: 28.91% complete, 0 mins, Avg loss: 0.23, Num correct: 874, Accuracy: 97.11%\n",
      "Epoch 38: 29.69% complete, 0 mins, Avg loss: 0.24, Num correct: 866, Accuracy: 96.22%\n",
      "Epoch 39: 30.47% complete, 0 mins, Avg loss: 0.23, Num correct: 863, Accuracy: 95.89%\n",
      "Epoch 40: 31.25% complete, 0 mins, Avg loss: 0.20, Num correct: 874, Accuracy: 97.11%\n",
      "\n",
      "Validation avg loss: 0.22, Num correct: 96, Accuracy: 96.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 41: 32.03% complete, 1 mins, Avg loss: 0.20, Num correct: 873, Accuracy: 97.00%\n",
      "Epoch 42: 32.81% complete, 1 mins, Avg loss: 0.23, Num correct: 865, Accuracy: 96.11%\n",
      "Epoch 43: 33.59% complete, 1 mins, Avg loss: 0.19, Num correct: 877, Accuracy: 97.44%\n",
      "Epoch 44: 34.38% complete, 1 mins, Avg loss: 0.21, Num correct: 870, Accuracy: 96.67%\n",
      "Epoch 45: 35.16% complete, 1 mins, Avg loss: 0.19, Num correct: 880, Accuracy: 97.78%\n",
      "\n",
      "Validation avg loss: 0.38, Num correct: 91, Accuracy: 91.00%\n",
      "Epoch 46: 35.94% complete, 1 mins, Avg loss: 0.17, Num correct: 879, Accuracy: 97.67%\n",
      "Epoch 47: 36.72% complete, 1 mins, Avg loss: 0.18, Num correct: 879, Accuracy: 97.67%\n",
      "Epoch 48: 37.50% complete, 1 mins, Avg loss: 0.16, Num correct: 882, Accuracy: 98.00%\n",
      "Epoch 49: 38.28% complete, 1 mins, Avg loss: 0.17, Num correct: 882, Accuracy: 98.00%\n",
      "Epoch 50: 39.06% complete, 1 mins, Avg loss: 0.15, Num correct: 888, Accuracy: 98.67%\n",
      "\n",
      "Validation avg loss: 0.23, Num correct: 95, Accuracy: 95.00%\n",
      "Epoch 51: 39.84% complete, 1 mins, Avg loss: 0.16, Num correct: 885, Accuracy: 98.33%\n",
      "Epoch 52: 40.62% complete, 1 mins, Avg loss: 0.15, Num correct: 890, Accuracy: 98.89%\n",
      "Epoch 53: 41.41% complete, 1 mins, Avg loss: 0.15, Num correct: 885, Accuracy: 98.33%\n",
      "Epoch 54: 42.19% complete, 1 mins, Avg loss: 0.14, Num correct: 891, Accuracy: 99.00%\n",
      "Epoch 55: 42.97% complete, 1 mins, Avg loss: 0.16, Num correct: 888, Accuracy: 98.67%\n",
      "\n",
      "Validation avg loss: 0.19, Num correct: 97, Accuracy: 97.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 56: 43.75% complete, 1 mins, Avg loss: 0.17, Num correct: 884, Accuracy: 98.22%\n",
      "Epoch 57: 44.53% complete, 1 mins, Avg loss: 0.17, Num correct: 886, Accuracy: 98.44%\n",
      "Epoch 58: 45.31% complete, 1 mins, Avg loss: 0.16, Num correct: 887, Accuracy: 98.56%\n",
      "Epoch 59: 46.09% complete, 1 mins, Avg loss: 0.15, Num correct: 891, Accuracy: 99.00%\n",
      "Epoch 60: 46.88% complete, 1 mins, Avg loss: 0.14, Num correct: 892, Accuracy: 99.11%\n",
      "\n",
      "Validation avg loss: 0.28, Num correct: 96, Accuracy: 96.00%\n",
      "Epoch 61: 47.66% complete, 1 mins, Avg loss: 0.13, Num correct: 887, Accuracy: 98.56%\n",
      "Epoch 62: 48.44% complete, 1 mins, Avg loss: 0.15, Num correct: 893, Accuracy: 99.22%\n",
      "Epoch 63: 49.22% complete, 1 mins, Avg loss: 0.16, Num correct: 878, Accuracy: 97.56%\n",
      "Epoch 64: 50.00% complete, 1 mins, Avg loss: 0.16, Num correct: 888, Accuracy: 98.67%\n",
      "Epoch 65: 50.78% complete, 1 mins, Avg loss: 0.14, Num correct: 890, Accuracy: 98.89%\n",
      "\n",
      "Validation avg loss: 0.30, Num correct: 95, Accuracy: 95.00%\n",
      "Epoch 66: 51.56% complete, 1 mins, Avg loss: 0.12, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 67: 52.34% complete, 1 mins, Avg loss: 0.14, Num correct: 892, Accuracy: 99.11%\n",
      "Epoch 68: 53.12% complete, 1 mins, Avg loss: 0.14, Num correct: 888, Accuracy: 98.67%\n",
      "Epoch 69: 53.91% complete, 1 mins, Avg loss: 0.13, Num correct: 892, Accuracy: 99.11%\n",
      "Epoch 70: 54.69% complete, 1 mins, Avg loss: 0.13, Num correct: 891, Accuracy: 99.00%\n",
      "\n",
      "Validation avg loss: 0.31, Num correct: 96, Accuracy: 96.00%\n",
      "Epoch 71: 55.47% complete, 1 mins, Avg loss: 0.12, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 72: 56.25% complete, 1 mins, Avg loss: 0.13, Num correct: 889, Accuracy: 98.78%\n",
      "Epoch 73: 57.03% complete, 1 mins, Avg loss: 0.12, Num correct: 893, Accuracy: 99.22%\n",
      "Epoch 74: 57.81% complete, 1 mins, Avg loss: 0.15, Num correct: 892, Accuracy: 99.11%\n",
      "Epoch 75: 58.59% complete, 1 mins, Avg loss: 0.12, Num correct: 894, Accuracy: 99.33%\n",
      "\n",
      "Validation avg loss: 0.31, Num correct: 97, Accuracy: 97.00%\n",
      "Epoch 76: 59.38% complete, 1 mins, Avg loss: 0.13, Num correct: 893, Accuracy: 99.22%\n",
      "Epoch 77: 60.16% complete, 1 mins, Avg loss: 0.19, Num correct: 880, Accuracy: 97.78%\n",
      "Epoch 78: 60.94% complete, 1 mins, Avg loss: 0.15, Num correct: 891, Accuracy: 99.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: 61.72% complete, 1 mins, Avg loss: 0.17, Num correct: 887, Accuracy: 98.56%\n",
      "Epoch 80: 62.50% complete, 1 mins, Avg loss: 0.16, Num correct: 889, Accuracy: 98.78%\n",
      "\n",
      "Validation avg loss: 0.36, Num correct: 97, Accuracy: 97.00%\n",
      "Epoch 81: 63.28% complete, 1 mins, Avg loss: 0.15, Num correct: 890, Accuracy: 98.89%\n",
      "Epoch 82: 64.06% complete, 1 mins, Avg loss: 0.15, Num correct: 889, Accuracy: 98.78%\n",
      "Epoch 83: 64.84% complete, 1 mins, Avg loss: 0.17, Num correct: 882, Accuracy: 98.00%\n",
      "Epoch 84: 65.62% complete, 1 mins, Avg loss: 0.16, Num correct: 887, Accuracy: 98.56%\n",
      "Epoch 85: 66.41% complete, 1 mins, Avg loss: 0.13, Num correct: 891, Accuracy: 99.00%\n",
      "\n",
      "Validation avg loss: 0.33, Num correct: 97, Accuracy: 97.00%\n",
      "Epoch 86: 67.19% complete, 1 mins, Avg loss: 0.12, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 87: 67.97% complete, 1 mins, Avg loss: 0.11, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 88: 68.75% complete, 1 mins, Avg loss: 0.14, Num correct: 892, Accuracy: 99.11%\n",
      "Epoch 89: 69.53% complete, 1 mins, Avg loss: 0.12, Num correct: 893, Accuracy: 99.22%\n",
      "Epoch 90: 70.31% complete, 1 mins, Avg loss: 0.15, Num correct: 889, Accuracy: 98.78%\n",
      "\n",
      "Validation avg loss: 0.20, Num correct: 98, Accuracy: 98.00%\n",
      "Epoch 91: 71.09% complete, 1 mins, Avg loss: 0.12, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 92: 71.88% complete, 1 mins, Avg loss: 0.13, Num correct: 893, Accuracy: 99.22%\n",
      "Epoch 93: 72.66% complete, 1 mins, Avg loss: 0.14, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 94: 73.44% complete, 1 mins, Avg loss: 0.13, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 95: 74.22% complete, 1 mins, Avg loss: 0.13, Num correct: 896, Accuracy: 99.56%\n",
      "\n",
      "Validation avg loss: 0.15, Num correct: 99, Accuracy: 99.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 96: 75.00% complete, 2 mins, Avg loss: 0.11, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 97: 75.78% complete, 2 mins, Avg loss: 0.11, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 98: 76.56% complete, 2 mins, Avg loss: 0.14, Num correct: 895, Accuracy: 99.44%\n",
      "Epoch 99: 77.34% complete, 2 mins, Avg loss: 0.12, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 100: 78.12% complete, 2 mins, Avg loss: 0.11, Num correct: 894, Accuracy: 99.33%\n",
      "\n",
      "Validation avg loss: 0.21, Num correct: 98, Accuracy: 98.00%\n",
      "Epoch 101: 78.91% complete, 2 mins, Avg loss: 0.12, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 102: 79.69% complete, 2 mins, Avg loss: 0.13, Num correct: 893, Accuracy: 99.22%\n",
      "Epoch 103: 80.47% complete, 2 mins, Avg loss: 0.11, Num correct: 895, Accuracy: 99.44%\n",
      "Epoch 104: 81.25% complete, 2 mins, Avg loss: 0.13, Num correct: 892, Accuracy: 99.11%\n",
      "Epoch 105: 82.03% complete, 2 mins, Avg loss: 0.10, Num correct: 899, Accuracy: 99.89%\n",
      "\n",
      "Validation avg loss: 0.13, Num correct: 99, Accuracy: 99.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 106: 82.81% complete, 2 mins, Avg loss: 0.10, Num correct: 900, Accuracy: 100.00%\n",
      "Epoch 107: 83.59% complete, 2 mins, Avg loss: 0.14, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 108: 84.38% complete, 2 mins, Avg loss: 0.12, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 109: 85.16% complete, 2 mins, Avg loss: 0.16, Num correct: 889, Accuracy: 98.78%\n",
      "Epoch 110: 85.94% complete, 2 mins, Avg loss: 0.12, Num correct: 893, Accuracy: 99.22%\n",
      "\n",
      "Validation avg loss: 0.15, Num correct: 97, Accuracy: 97.00%\n",
      "Epoch 111: 86.72% complete, 2 mins, Avg loss: 0.12, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 112: 87.50% complete, 2 mins, Avg loss: 0.11, Num correct: 897, Accuracy: 99.67%\n",
      "Epoch 113: 88.28% complete, 2 mins, Avg loss: 0.12, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 114: 89.06% complete, 2 mins, Avg loss: 0.11, Num correct: 898, Accuracy: 99.78%\n",
      "Epoch 115: 89.84% complete, 2 mins, Avg loss: 0.11, Num correct: 896, Accuracy: 99.56%\n",
      "\n",
      "Validation avg loss: 0.11, Num correct: 99, Accuracy: 99.00%\n",
      "Model saved in file: ../saved-models/task-1.ckpt\n",
      "\n",
      "Epoch 116: 90.62% complete, 2 mins, Avg loss: 0.10, Num correct: 900, Accuracy: 100.00%\n",
      "Epoch 117: 91.41% complete, 2 mins, Avg loss: 0.11, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 118: 92.19% complete, 2 mins, Avg loss: 0.11, Num correct: 898, Accuracy: 99.78%\n",
      "Epoch 119: 92.97% complete, 2 mins, Avg loss: 0.11, Num correct: 897, Accuracy: 99.67%\n",
      "Epoch 120: 93.75% complete, 2 mins, Avg loss: 0.11, Num correct: 897, Accuracy: 99.67%\n",
      "\n",
      "Validation avg loss: 0.15, Num correct: 98, Accuracy: 98.00%\n",
      "Epoch 121: 94.53% complete, 2 mins, Avg loss: 0.12, Num correct: 896, Accuracy: 99.56%\n",
      "Epoch 122: 95.31% complete, 2 mins, Avg loss: 0.10, Num correct: 898, Accuracy: 99.78%\n",
      "Epoch 123: 96.09% complete, 2 mins, Avg loss: 0.11, Num correct: 898, Accuracy: 99.78%\n",
      "Epoch 124: 96.88% complete, 2 mins, Avg loss: 0.10, Num correct: 899, Accuracy: 99.89%\n",
      "Epoch 125: 97.66% complete, 2 mins, Avg loss: 0.10, Num correct: 899, Accuracy: 99.89%\n",
      "\n",
      "Validation avg loss: 0.13, Num correct: 98, Accuracy: 98.00%\n",
      "Epoch 126: 98.44% complete, 2 mins, Avg loss: 0.13, Num correct: 894, Accuracy: 99.33%\n",
      "Epoch 127: 99.22% complete, 2 mins, Avg loss: 0.14, Num correct: 889, Accuracy: 98.78%\n",
      "Duration: 2 mins\n"
     ]
    }
   ],
   "source": [
    "model = dmn_plus(task = 1)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get test error after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../saved-models/task-1.ckpt\n",
      "Model restored\n",
      "0 mins, Avg loss: 0.19, Num correct: 977, Accuracy: 97.70%\n"
     ]
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get word answer for input-question pairs (in word form) for GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garden']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.answer_user_data(inputs = [\"John went to the bathroom. Adam went to the office. John went back to the plaza.\"], questions = [\"Where is John?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../saved-models/task-1.ckpt\n",
      "Model restored\n",
      "INFO:tensorflow:Restoring parameters from ../saved-models/task-1.ckpt\n",
      "Model restored\n",
      "0 mins, Avg loss: 0.24, Num correct: 970, Accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "model = dmn_plus(task = 1)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
