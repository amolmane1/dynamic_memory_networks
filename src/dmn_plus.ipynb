{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import os.path\n",
    "\n",
    "def load_babi_data(is_training, task, embedding_dim, embedding, word_id_dict):\n",
    "    if is_training:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\"\n",
    "    else:\n",
    "        filepath = \"../../../datasets/facebook_babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_test.txt\"\n",
    "    \n",
    "    file = open(filepath)\n",
    "    num_words_in_longest_input_sentence = 0\n",
    "    num_words_in_longest_question = 0\n",
    "    num_sentences_in_each_chapter = []\n",
    "    chapter_input = []\n",
    "    data = []\n",
    "    \n",
    "    for line in file:\n",
    "        items = re.sub('[?.]', '', line).lower().split()\n",
    "        if items[-1].isdigit():\n",
    "            data.append({'I': copy.deepcopy(chapter_input),\n",
    "                     'Q': items[1:-2],\n",
    "                     'A': [items[-2]]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "            num_words_in_longest_question = max(num_words_in_longest_question, len(items[1:-2]))\n",
    "        else:\n",
    "            if items[0] == '1':\n",
    "                chapter_input = [items[1:]]\n",
    "            else:\n",
    "                chapter_input.append(items[1:])\n",
    "            num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, len(items[1:]))\n",
    "    file.close()\n",
    "\n",
    "    num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "    num_chapters = len(data)\n",
    "\n",
    "    data_inputs = np.zeros([num_chapters, num_sentences_in_longest_input, num_words_in_longest_input_sentence, embedding_dim])\n",
    "    data_questions = np.zeros([num_chapters, num_words_in_longest_question, embedding_dim])\n",
    "    data_answers = np.zeros([num_chapters])\n",
    "    for chapter_index, chapter in enumerate(data):\n",
    "        for sentence_index, sentence in enumerate(chapter['I']):\n",
    "            data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = embedding[[word_id_dict[word] for word in sentence]]\n",
    "        data_questions[chapter_index, 0:len(chapter['Q']), :] = embedding[[word_id_dict[word] for word in chapter['Q']]]\n",
    "        data_answers[chapter_index] = word_id_dict[chapter['A'][0]]\n",
    "    \n",
    "    return(data_inputs, data_questions, data_answers, \n",
    "           num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "           num_words_in_longest_question, num_sentences_in_longest_input, \n",
    "           num_chapters)\n",
    "\n",
    "def load_glove():\n",
    "    vocab_size = 400000\n",
    "    embedding_dim = 50\n",
    "    file = open(\"../../../datasets/glove_6b/glove.6B.50d.txt\")    \n",
    "    embedding = np.ndarray([vocab_size, embedding_dim])\n",
    "    word_id_dict = {}\n",
    "    id_word_dict = {}\n",
    "    id = 0\n",
    "    for line in file:\n",
    "        items = line.split(' ')\n",
    "        word_id_dict[items[0]] = id\n",
    "        id_word_dict[id] = items[0]\n",
    "        embedding[id,:] = np.array([float(i) for i in items[1:]])\n",
    "        id += 1\n",
    "    file.close()\n",
    "    return(embedding, word_id_dict, id_word_dict, vocab_size, embedding_dim)\n",
    "\n",
    "def create_position_encoding(embedding_dim, num_words_in_longest_input_sentence):\n",
    "    ## Position encoding\n",
    "    position_encoding = np.ones([embedding_dim, num_words_in_longest_input_sentence], dtype=np.float32)\n",
    "\n",
    "    ## Below (my implementation, from section 3.1 in https://arxiv.org/pdf/1603.01417.pdf) didn't work.\n",
    "    # for j in range(1, num_words_in_longest_input_sentence+1):\n",
    "    #     for d in range(1, embedding_dim+1):\n",
    "    #         position_encoding[d-1, j-1] = (1 - j/num_words_in_longest_input_sentence) - (d/embedding_dim)*(1 - 2*j/num_words_in_longest_input_sentence)\n",
    "\n",
    "    ## Copied from https://github.com/domluna/memn2n\n",
    "    ls = num_words_in_longest_input_sentence+1\n",
    "    le = embedding_dim+1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            position_encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)\n",
    "    position_encoding = 1 + 4 * position_encoding / embedding_dim / num_words_in_longest_input_sentence\n",
    "    position_encoding = np.transpose(position_encoding)\n",
    "    return(position_encoding)\n",
    "\n",
    "\n",
    "class dmn_plus:\n",
    "    def __init__(self):\n",
    "        self.hidden_layer_size = 80\n",
    "        self.num_steps = 3\n",
    "        self.batch_size = 100\n",
    "        self.dropout_probability = 0.9\n",
    "        self.l2_regularization_lambda = 0.001\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 30\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        self.embedding, self.word_id_dict, self.id_word_dict, self.vocab_size, self.embedding_dim = load_glove()\n",
    "        \n",
    "    def load_data(self, is_training = True, task = 1):\n",
    "        self.task = task\n",
    "        self.data_inputs, self.data_questions, self.data_answers, self.num_sentences_in_each_chapter, self.num_words_in_longest_input_sentence, self.num_words_in_longest_question, self.num_sentences_in_longest_input, self.num_chapters = load_babi_data(is_training, self.task, self.embedding_dim, self.embedding, self.word_id_dict)\n",
    "        self.position_encoding = create_position_encoding(self.embedding_dim, self.num_words_in_longest_input_sentence)\n",
    "        \n",
    "    def prepare_user_inputted_data(self, inputs, questions):    \n",
    "        num_words_in_longest_input_sentence = 0\n",
    "        num_words_in_longest_question = 0\n",
    "        num_sentences_in_each_chapter = []\n",
    "        chapter_input = []\n",
    "        data = []\n",
    "\n",
    "        for index in range(len(inputs)):\n",
    "            input = re.sub('[?]', '', inputs[index]).lower().split('.')\n",
    "            chapter_input = []\n",
    "            for sentence in input:\n",
    "                if sentence != '':\n",
    "                    chapter_input.append(sentence.split())\n",
    "            cleaned_question = re.sub('[?]', '', questions[index]).lower().split()\n",
    "            data.append({'I': chapter_input,\n",
    "                         'Q': cleaned_question,\n",
    "                         'A': [None]})\n",
    "            num_sentences_in_each_chapter.append(len(chapter_input))\n",
    "            num_words_in_longest_question = max(num_words_in_longest_question, len(cleaned_question))\n",
    "            num_words_in_longest_input_sentence = max(num_words_in_longest_input_sentence, len(chapter_input))\n",
    "        num_sentences_in_longest_input = max(num_sentences_in_each_chapter)\n",
    "        num_chapters = len(data)\n",
    "\n",
    "        data_inputs = np.zeros([num_chapters, num_sentences_in_longest_input, num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        data_questions = np.zeros([num_chapters, num_words_in_longest_question, self.embedding_dim])\n",
    "        data_answers = np.zeros([num_chapters])\n",
    "        for chapter_index, chapter in enumerate(data):\n",
    "            for sentence_index, sentence in enumerate(chapter['I']):\n",
    "                data_inputs[chapter_index, sentence_index, 0:len(sentence), :] = self.embedding[[self.word_id_dict[word] for word in sentence]]\n",
    "            data_questions[chapter_index, 0:len(chapter['Q']), :] = self.embedding[[self.word_id_dict[word] for word in chapter['Q']]]\n",
    "            data_answers[chapter_index] = None\n",
    "\n",
    "        return(data_inputs, data_questions, data_answers, \n",
    "               num_sentences_in_each_chapter, num_words_in_longest_input_sentence,\n",
    "               num_words_in_longest_question, num_sentences_in_longest_input, \n",
    "               num_chapters)\n",
    "        \n",
    "\n",
    "    def get_batch(self, batch_number):\n",
    "        return {self.inputs: self.data_inputs[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.questions: self.data_questions[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.answers: self.data_answers[batch_number*self.batch_size: (batch_number+1)*self.batch_size],\n",
    "                self.input_lengths: self.num_sentences_in_each_chapter[batch_number*self.batch_size: (batch_number+1)*self.batch_size]\n",
    "               }\n",
    "    \n",
    "    def random_entry_from_data(self):\n",
    "        idx = np.random.randint(0, self.num_chapters)\n",
    "        return {self.inputs: np.expand_dims(self.data_inputs[idx], axis = 0),\n",
    "                self.questions: np.expand_dims(self.data_questions[idx], axis = 0),\n",
    "                self.answers: np.expand_dims(self.data_answers[idx], axis = 0),\n",
    "                self.input_lengths: np.expand_dims(self.num_sentences_in_each_chapter[idx], axis = 0)\n",
    "               }\n",
    "    \n",
    "    def sgd_train(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss, epoch_num_correct, _ = self.sess.run((self.loss, self.num_correct, self.optimizer), \n",
    "                                                                            feed_dict = self.random_entry_from_data())\n",
    "            if epoch%100 == 0:\n",
    "                print(\"Epoch %d: %.2f%% complete, %d mins, Loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/self.num_chapters))\n",
    "        end_time = time.time()\n",
    "        print(\"Duration: %d mins\" % int((end_time - start_time)/60))\n",
    "                \n",
    "    def train(self):            \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = epoch_num_correct = 0\n",
    "            for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "                batch_loss, batch_num_correct, _ = self.sess.run((self.loss, self.num_correct, self.optimizer), \n",
    "                                                                            feed_dict = self.get_batch(batch_idx))\n",
    "                epoch_loss += batch_loss\n",
    "                epoch_num_correct += batch_num_correct\n",
    "            print(\"Epoch %d: %.2f%% complete, %d mins, Loss: %.2f, Num correct: %d, Accuracy: %.2f%%\" % (epoch, \n",
    "                                                                                   epoch*100.0/self.num_epochs,\n",
    "                                                                                    (time.time() - start_time)/60,\n",
    "                                                                                   epoch_loss, \n",
    "                                                                                    epoch_num_correct,\n",
    "                                                                                    epoch_num_correct*100.0/self.num_chapters))\n",
    "        end_time = time.time()\n",
    "        print(\"Duration: %d mins\" % int((end_time - start_time)/60))\n",
    "        \n",
    "    def save(self):\n",
    "        # create filename based on task\n",
    "        save_path = self.saver.save(self.sess, \"../saved-models/task-%d.ckpt\"%self.task)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "    def restore(self, task):\n",
    "        # check if there is any saved model for that particular task\n",
    "        if os.path.isfile(\"../saved-models/task-%d.ckpt.meta\"%task):\n",
    "            self.saver.restore(self.sess, \"../saved-models/task-%d.ckpt\"%task)\n",
    "            print(\"Model restored\")\n",
    "        else:\n",
    "            print(\"Saved model for given task does not exist\")\n",
    "        \n",
    "    def test(self):\n",
    "        start_time = time.time()\n",
    "        total_num_correct = 0\n",
    "        for batch_idx in range(self.num_chapters/self.batch_size):\n",
    "            batch_num_correct = self.sess.run(self.num_correct, feed_dict = self.get_batch(batch_idx))\n",
    "            total_num_correct += batch_num_correct\n",
    "        print(\"%d mins, Num correct: %d, Accuracy: %.2f%%\" % ((time.time() - start_time)/60,\n",
    "                                                              total_num_correct,\n",
    "                                                              total_num_correct*100.0/self.num_chapters))\n",
    "        \n",
    "    def create_tensorflow_graph(self):\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, self.num_sentences_in_longest_input, self.num_words_in_longest_input_sentence, self.embedding_dim])\n",
    "        self.questions = tf.placeholder(tf.float32, [None, self.num_words_in_longest_question, self.embedding_dim])\n",
    "        self.answers = tf.placeholder(tf.int32, [None])\n",
    "        self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        ## Question module\n",
    "        with tf.variable_scope('question_module'):\n",
    "            question_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            _, self.question_vector = tf.nn.dynamic_rnn(question_gru_cell,\n",
    "                                                  self.questions,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "        ## Input module\n",
    "        with tf.variable_scope('input_module'):\n",
    "\n",
    "            positionally_encoded_inputs = tf.reduce_sum(self.inputs*self.position_encoding, 2)\n",
    "\n",
    "            input_forward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_backward_gru_cell = tf.contrib.rnn.GRUCell(self.hidden_layer_size)\n",
    "            input_module_output, _ = tf.nn.bidirectional_dynamic_rnn(input_forward_gru_cell,\n",
    "                                                                    input_backward_gru_cell,\n",
    "                                                                    positionally_encoded_inputs,\n",
    "                                                                    sequence_length = self.input_lengths,\n",
    "                                                                    dtype = tf.float32)\n",
    "            input_fact_vectors = tf.add(input_module_output[0], input_module_output[1])\n",
    "            input_fact_vectors = tf.nn.dropout(input_fact_vectors, self.dropout_probability)\n",
    "\n",
    "        ## Episodic Memory module\n",
    "        with tf.variable_scope('episodic_memory_module'):\n",
    "            weight = tf.get_variable(\"weight\", [3*self.hidden_layer_size, 80],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            bias = tf.get_variable(\"bias\", [1, self.hidden_layer_size],\n",
    "                                            initializer=tf.random_normal_initializer())\n",
    "            self.previous_memory = self.question_vector\n",
    "            for step in range(self.num_steps):\n",
    "                attentions = []\n",
    "                for fact_index, fact_vector in enumerate(tf.unstack(input_fact_vectors, axis = 1)):\n",
    "#                     if fact_index == 0:\n",
    "                    reuse = bool(step) or bool(fact_index)\n",
    "                    with tf.variable_scope(\"attention\", reuse = reuse):\n",
    "                        z = tf.concat([tf.multiply(fact_vector, self.question_vector), \n",
    "                                       tf.multiply(fact_vector, self.previous_memory),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.question_vector)),\n",
    "                                       tf.abs(tf.subtract(fact_vector, self.previous_memory))], 1)\n",
    "                        attention = tf.contrib.layers.fully_connected(z,\n",
    "                                                                    self.embedding_dim,\n",
    "                                                                    activation_fn=tf.nn.tanh,\n",
    "                                                                    reuse=reuse, scope=\"fc1\")\n",
    "                        attention = tf.contrib.layers.fully_connected(attention,\n",
    "                                                                    1,\n",
    "                                                                    activation_fn=None,\n",
    "                                                                    reuse=reuse, scope=\"fc2\")\n",
    "                        attentions.append(tf.squeeze(attention))\n",
    "                attentions = tf.expand_dims(tf.nn.softmax(tf.transpose(tf.stack(attentions))), axis=-1)\n",
    "                reuse = True if step > 0 else False\n",
    "                # soft attention\n",
    "                self.context_vector = tf.reduce_sum(tf.multiply(input_fact_vectors, attentions), axis = 1)\n",
    "#                 with tf.variable_scope(\"step%d\"%step):\n",
    "                self.previous_memory = tf.nn.relu(tf.matmul(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1), \n",
    "                                                            weight) + bias)\n",
    "#                     self.previous_memory = tf.contrib.layers.fully_connected(tf.concat([self.previous_memory, self.context_vector, self.question_vector], axis = 1),\n",
    "#                                                                                 self.hidden_layer_size,\n",
    "#                                                                                 activation_fn=tf.nn.relu)\n",
    "                        \n",
    "            self.previous_memory = tf.nn.dropout(self.previous_memory, self.dropout_probability)\n",
    "\n",
    "        ## Answer module\n",
    "        with tf.variable_scope('answer_module') as scope:\n",
    "            logits = tf.contrib.layers.fully_connected(inputs = tf.concat([self.previous_memory, self.question_vector], axis = 1),\n",
    "                                                      num_outputs = self.vocab_size,\n",
    "                                                      activation_fn = None)\n",
    "\n",
    "            ## Loss and metrics\n",
    "            self.loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = self.answers))\n",
    "\n",
    "            # add l2 regularization for all variables except biases\n",
    "            for v in tf.trainable_variables():\n",
    "                if not 'bias' in v.name.lower():\n",
    "                    self.loss += self.l2_regularization_lambda * tf.nn.l2_loss(v)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            prediction = tf.cast(tf.argmax(tf.nn.softmax(logits), 1), 'int32')\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(tf.equal(prediction, self.answers), tf.int32))\n",
    "            \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = dmn_plus()\n",
    "model.load_embeddings()\n",
    "model.load_data(is_training=True, task=1)\n",
    "model.create_tensorflow_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.00% complete, 0 mins, Loss: 4831.61, Num correct: 156, Accuracy: 15.60%\n",
      "Epoch 1: 3.33% complete, 0 mins, Loss: 2059.45, Num correct: 204, Accuracy: 20.40%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-22a9f6312119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-64b3d107055e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_chapters\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 batch_loss, batch_num_correct, _ = self.sess.run((self.loss, self.num_correct, self.optimizer), \n\u001b[0;32m--> 187\u001b[0;31m                                                                             feed_dict = self.get_batch(batch_idx))\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mepoch_num_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_num_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amolmane1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 80)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sess.run(tf.global_variables_initializer())\n",
    "res = model.sess.run((model.previous_memory), feed_dict = model.get_batch(0))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ../saved-models/task-1.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mins, Num correct: 730, Accuracy: 73.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_data(is_training=False, task=1)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get test error after loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../saved-models/task-1.ckpt\n",
      "Model restored\n",
      "0 mins, Num correct: 990, Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "model = dmn_plus()\n",
    "model.load_embeddings()\n",
    "model.load_data(is_training=False, task=1)\n",
    "model.create_tensorflow_graph()\n",
    "model.restore(task = 1)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To get word answer for input-question pairs (in word form) for GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-dcd7d591fbf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepare_data_for_dmn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"John went to the bathroom. Mary went to the office.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Where is Mary?\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-8ee7bc6d6468>\u001b[0m in \u001b[0;36mprepare_data_for_dmn\u001b[0;34m(inputs, questions)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchapter_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mdata_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchapter_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mdata_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchapter_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchapter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdata_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchapter_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_id_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchapter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'embedding' is not defined"
     ]
    }
   ],
   "source": [
    "res = model.prepare_data_for_dmn([\"John went to the bathroom. Mary went to the office.\"], [\"Where is Mary?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(self, input, question):\n",
    "    # convert inputs into embeddings\n",
    "    \n",
    "    # load model on inputs\n",
    "    \n",
    "    # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
